{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world!',\n",
       " 'This is a sample text for natural language processing.',\n",
       " \"We are going to perform tokenization on it using NLTK's tokenizer functionality.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"Hello world! This is a sample text for natural language processing. \n",
    "We are going to perform tokenization on it using NLTK's tokenizer functionality.\"\"\"\n",
    "# tokenizes by sentence.\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) # number of sentences in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.', 'We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'s\", 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) # number of words and punctuations in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!']\n",
      "['This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.']\n",
      "['We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'s\", 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "# print words of each sentence\n",
    "for sentence in sentences:\n",
    "    print(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.', 'We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'\", 's', 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between the above word_tokenize and wordpunct_tokenize is that in the previous example the apostrophe _**\" ' \"**_ did non get split separately. But in this example, it got separated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'it',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " \"'s\",\n",
       " 'tokenizer',\n",
       " 'functionality',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides several word tokenizers, each suited for different use cases. Here are the key tokenizers and their differences:\n",
    "\n",
    "##### 1. **`word_tokenize` (Recommended)**\n",
    "   - **Implementation:** Uses the Penn Treebank tokenizer from `nltk.tokenize.punkt`.\n",
    "   - **Features:** Handles punctuation, contractions, and special cases like \"U.S.\" correctly.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import word_tokenize\n",
    "     text = \"I'm going to the U.S. next week!\"\n",
    "     print(word_tokenize(text))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     [\"I\", \"'m\", \"going\", \"to\", \"the\", \"U.S.\", \"next\", \"week\", \"!\"]\n",
    "     ```\n",
    "   - **Use Case:** General-purpose word tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **`TreebankWordTokenizer`**\n",
    "   - **Implementation:** Uses the Penn Treebank tokenizer rules (same as `word_tokenize`).\n",
    "   - **Features:** Splits contractions (e.g., \"can't\" → [\"ca\", \"n't\"]), handles punctuation.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import TreebankWordTokenizer\n",
    "     tokenizer = TreebankWordTokenizer()\n",
    "     print(tokenizer.tokenize(\"Can't won't don't\"))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     ['Ca', \"n't\", 'wo', \"n't\", 'do', \"n't\"]\n",
    "     ```\n",
    "   - **Use Case:** When working with text where contractions need to be split.\n",
    "\n",
    "A **contraction** is a shortened form of one or more words where missing letters are replaced by an apostrophe (`'`). Contractions are commonly used in informal writing and speech.  \n",
    "\n",
    "##### **Examples of Contractions:**\n",
    "| Full Form | Contraction |\n",
    "|-----------|------------|\n",
    "| I am | I'm |\n",
    "| You are | You're |\n",
    "| He is / He has | He's |\n",
    "| They are | They're |\n",
    "| Cannot | Can't |\n",
    "| Will not | Won't |\n",
    "| Do not | Don't |\n",
    "| Should not | Shouldn't |\n",
    "| Would have | Would've |\n",
    "\n",
    "##### **Why Do Contractions Matter in NLP?**\n",
    "- Some tokenizers **split contractions** into separate words (`\"can't\"` → `[\"ca\", \"n't\"]`).\n",
    "- Others **keep contractions intact** (`\"can't\"` → `[\"can't\"]`).\n",
    "- Handling contractions correctly is important for **sentiment analysis**, **text preprocessing**, and **machine learning models**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### 3. **`WordPunctTokenizer`**\n",
    "   - **Implementation:** Splits words and punctuation separately.\n",
    "   - **Features:** Breaks contractions into separate parts (e.g., \"can't\" → [\"can\", \"'t\"]).\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import WordPunctTokenizer\n",
    "     tokenizer = WordPunctTokenizer()\n",
    "     print(tokenizer.tokenize(\"I'm excited!\"))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     ['I', \"'\", 'm', 'excited', '!']\n",
    "     ```\n",
    "   - **Use Case:** When punctuation needs to be treated as separate tokens.\n",
    "\n",
    "---\n",
    "There are a couple other tokenizers.\n",
    "\n",
    "##### **Summary**\n",
    "| Tokenizer | Handles Contractions? | Splits Punctuation? | Use Case |\n",
    "|-----------|------------------|----------------|---------|\n",
    "| `word_tokenize` | Yes | Mostly | General NLP tasks |\n",
    "| `TreebankWordTokenizer` | Yes (splits aggressively) | Yes | Similar to `word_tokenize`, more aggressive |\n",
    "| `ToktokTokenizer` | No | Yes | Fast and simple tokenization |\n",
    "| `RegexpTokenizer` | Custom | Custom | Custom rules-based tokenization |\n",
    "| `WordPunctTokenizer` | Yes (aggressive) | Yes (separates all punctuation) | When punctuation should be separate |\n",
    "| `MWETokenizer` | No | No | Preserve multi-word expressions |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** is the process of reducing a word to its root form (also called a \"stem\") by **removing suffixes**. It helps normalize words so that variations of a word are treated as the same.  \n",
    "\n",
    "For example:  \n",
    "- **\"running\" → \"run\"**  \n",
    "- **\"flies\" → \"fli\"** (incorrect but common with some stemmers)  \n",
    "- **\"happily\" → \"happili\"**  \n",
    "\n",
    "Stemming is a **rule-based** approach and doesn't always produce real words. It just chops off endings based on predefined rules.\n",
    "\n",
    "---\n",
    "\n",
    "**Limitations of Stemming**\n",
    "- Doesn't always produce real words (e.g., *\"flies\"* → *\"fli\"*)  \n",
    "- Different words may map to the same stem incorrectly  \n",
    "- Over-stemming (too aggressive) or under-stemming (not aggressive enough)  \n",
    "\n",
    "---\n",
    "\n",
    " **Stemming vs Lemmatization**\n",
    "If you need more **accurate** root words, **lemmatization** is a better choice because it uses a **dictionary-based** approach instead of just chopping off suffixes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating → eat\n",
      "eats → eat\n",
      "eaten → eaten\n",
      "writing → write\n",
      "writes → write\n",
      "programming → program\n",
      "programs → program\n",
      "history → histori\n",
      "finally → final\n",
      "finalized → final\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word, \"→\", porter_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change from \"history\" --> \"histori\" exhibits the major disadvantage of Stemming. The meaning of the original word has changed completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "porter_stemmer.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a word. The original word lost its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runn\n",
      "happi\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# Regexp Stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Define a regex pattern to remove common suffixes (-ing, -ed, -ly)\n",
    "regexp_stemmer = RegexpStemmer(r'ing$|ed$|ly$', min=4)\n",
    "\n",
    "print(regexp_stemmer.stem(\"running\"))   # Output: runn\n",
    "print(regexp_stemmer.stem(\"happily\"))   # Output: happi\n",
    "print(regexp_stemmer.stem(\"worked\"))    # Output: work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "ingeat\n"
     ]
    }
   ],
   "source": [
    "regexp_stemmer = RegexpStemmer('ing|ed$|ly$')\n",
    "print(regexp_stemmer.stem(\"ingeating\")) #eat\n",
    "regexp_stemmer = RegexpStemmer('ing$|ed$|ly$')\n",
    "print(regexp_stemmer.stem(\"ingeating\")) #ingeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happili\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "print(snowball_stemmer.stem(\"happily\"))\n",
    "print(snowball_stemmer.stem(\"worked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating → eat\n",
      "eats → eat\n",
      "eaten → eaten\n",
      "writing → write\n",
      "writes → write\n",
      "programming → program\n",
      "programs → program\n",
      "history → histori\n",
      "finally → final\n",
      "finalized → final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"→\", snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing all three stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem(\"fairly\"), porter_stemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sporting')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_stemmer.stem(\"fairly\"), regexp_stemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem(\"fairly\"), snowball_stemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1️⃣ Porter Stemmer**\n",
    "\n",
    "✅ **Pros:**  \n",
    "- One of the most widely used stemming algorithms.  \n",
    "- Uses a **set of heuristic rules** to remove common suffixes.  \n",
    "- Efficient and relatively fast.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Sometimes over-stems words (e.g., `\"flies\"` → `\"fli\"`).  \n",
    "- Does not always produce real words.  \n",
    "- **Not customizable** (fixed rules).  \n",
    "\n",
    "---\n",
    "\n",
    "**2️⃣ Regex Stemmer**\n",
    "\n",
    "✅ **Pros:**  \n",
    "- **Customizable**: You define the regex pattern to remove suffixes.  \n",
    "- Useful for **domain-specific** text processing.  \n",
    "- Can prevent over-stemming by **setting minimum word length** (`min=` parameter).  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Requires **manual regex tuning** for different datasets.  \n",
    "- May **miss irregular word forms** (e.g., `\"better\"` won’t stem to `\"good\"`).  \n",
    "- **Not language-aware** (simply removes predefined suffixes).  \n",
    "---\n",
    "\n",
    "**3️⃣ Snowball Stemmer**\n",
    "\n",
    "✅ **Pros:**  \n",
    "- **Improved version of Porter Stemmer**.  \n",
    "- Supports **multiple languages** (e.g., English, French, Spanish).  \n",
    "- More **accurate and flexible** than Porter.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- **Slower than Porter Stemmer** due to additional rules.  \n",
    "- Not as customizable as Regex Stemmer.  \n",
    "\n",
    "---\n",
    "\n",
    "**Comparison Table**\n",
    "\n",
    "| Feature          | **Porter Stemmer** | **Regex Stemmer** | **Snowball Stemmer** |\n",
    "|-----------------|------------------|-----------------|------------------|\n",
    "| **Algorithm**   | Rule-based       | Regex-based    | Rule-based (Improved Porter) |\n",
    "| **Customizable?** | ❌ No | ✅ Yes | ❌ No |\n",
    "| **Language Support** | ❌ English Only | ❌ Manual | ✅ Multiple Languages |\n",
    "| **Speed** | ✅ Fast | ✅ Fast | ❌ Slower (More Rules) |\n",
    "| **Accuracy** | ❌ Can over-stem | ✅ Depends on Regex | ✅ More accurate than Porter |\n",
    "| **Best Use Case** | General NLP tasks | Domain-specific text | Multi-language support |\n",
    "\n",
    "---\n",
    "\n",
    "**Which One Should You Use?**\n",
    "\n",
    "🔹 **Use Porter Stemmer** → If you want a simple, fast stemming method for **English text**.  \n",
    "🔹 **Use Regex Stemmer** → If you need **full control** over stemming rules for **custom datasets**.  \n",
    "🔹 **Use Snowball Stemmer** → If you want **better accuracy** and support for **multiple languages**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** is the process of reducing a word to its **base or dictionary form (lemma)** while ensuring it remains a real word. Unlike **stemming**, which just chops off suffixes, lemmatization considers **grammatical meaning** using a **lexical database** like WordNet.\n",
    "\n",
    "---\n",
    "\n",
    "**How Does Lemmatization Work?**\n",
    "\n",
    "- **Considers the context** and part of speech (POS) of a word.  \n",
    "- Uses a **dictionary lookup** to find the root form (lemma).  \n",
    "- Ensures the output is a valid word.  \n",
    "\n",
    "🔹 **Example:**  \n",
    "| Word | Stemmed (Porter) | Lemmatized (WordNet) |\n",
    "|------|----------------|------------------|\n",
    "| Running | run | run |\n",
    "| Better | better | good |\n",
    "| Studies | studi | study |\n",
    "| Mice | mice | mouse |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔹 **Explanation:**  \n",
    "\n",
    "- `\"running\"` (verb) → `\"run\"` ✅  \n",
    "- `\"better\"` (adjective) → `\"good\"` ✅  \n",
    "- `\"mice\"` (noun) → `\"mouse\"` ✅  \n",
    "- `\"studies\"` (noun) → `\"study\"` ✅  \n",
    "\n",
    "🚨 **POS Tagging is Important!**  \n",
    "\n",
    "- If no `pos` is provided, it assumes the word is a **noun**.  \n",
    "- `\"running\"` → `\"running\"` (incorrect)  \n",
    "- `\"running\", pos=\"v\"` → `\"run\"` (correct)  \n",
    "\n",
    "---\n",
    "\n",
    "**Stemming vs Lemmatization**\n",
    "\n",
    "| Feature | **Stemming** | **Lemmatization** |\n",
    "|---------|-------------|------------------|\n",
    "| **Method** | Removes suffixes | Uses dictionary lookup |\n",
    "| **Grammar Aware?** | ❌ No | ✅ Yes |\n",
    "| **Produces Real Words?** | ❌ No | ✅ Yes |\n",
    "| **Computational Cost** | ✅ Fast | ❌ Slower |\n",
    "| **Example (\"better\")** | **\"better\"** → **\"better\"** | **\"better\"** → **\"good\"** |\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use Lemmatization?**\n",
    "\n",
    "✅ **Linguistic Accuracy Required** (e.g., chatbots, search engines).  \n",
    "✅ **When Meaning Matters** (e.g., sentiment analysis).  \n",
    "✅ **If You Have Computational Resources** (since it's slower than stemming).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating → eating\n",
      "eats → eats\n",
      "eaten → eaten\n",
      "writing → writing\n",
      "writes → writes\n",
      "programming → programming\n",
      "programs → program\n",
      "history → history\n",
      "finally → finally\n",
      "finalized → finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"→\", lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes as all these words exist in the dictionary. Now we will add a \"pos\" attribute in the lemmatize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating → eat\n",
      "eats → eat\n",
      "eaten → eat\n",
      "writing → write\n",
      "writes → write\n",
      "programming → program\n",
      "programs → program\n",
      "history → history\n",
      "finally → finally\n",
      "finalized → finalize\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Pos Tagging\n",
    "Noun- n\n",
    "Verb- v\n",
    "Adjective- a\n",
    "Adverb- r\n",
    "'''\n",
    "# by default it's noun\n",
    "for word in words:\n",
    "    print(word, \"→\", lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating → eating\n",
      "eats → eats\n",
      "eaten → eaten\n",
      "writing → writing\n",
      "writes → writes\n",
      "programming → programming\n",
      "programs → programs\n",
      "history → history\n",
      "finally → finally\n",
      "finalized → finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"→\", lemmatizer.lemmatize(word, pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\", pos='a'), lemmatizer.lemmatize(\"sportingly\", pos = 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage of Lemmatization over Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem(\"better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"better\", 'a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samarth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
