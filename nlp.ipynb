{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world!',\n",
       " 'This is a sample text for natural language processing.',\n",
       " \"We are going to perform tokenization on it using NLTK's tokenizer functionality.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"Hello world! This is a sample text for natural language processing. \n",
    "We are going to perform tokenization on it using NLTK's tokenizer functionality.\"\"\"\n",
    "# tokenizes by sentence.\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) # number of sentences in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.', 'We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'s\", 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) # number of words and punctuations in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!']\n",
      "['This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.']\n",
      "['We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'s\", 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "# print words of each sentence\n",
    "for sentence in sentences:\n",
    "    print(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.', 'We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'\", 's', 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between the above word_tokenize and wordpunct_tokenize is that in the previous example the apostrophe _**\" ' \"**_ did non get split separately. But in this example, it got separated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'it',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " \"'s\",\n",
       " 'tokenizer',\n",
       " 'functionality',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides several word tokenizers, each suited for different use cases. Here are the key tokenizers and their differences:\n",
    "\n",
    "##### 1. **`word_tokenize` (Recommended)**\n",
    "   - **Implementation:** Uses the Penn Treebank tokenizer from `nltk.tokenize.punkt`.\n",
    "   - **Features:** Handles punctuation, contractions, and special cases like \"U.S.\" correctly.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import word_tokenize\n",
    "     text = \"I'm going to the U.S. next week!\"\n",
    "     print(word_tokenize(text))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     [\"I\", \"'m\", \"going\", \"to\", \"the\", \"U.S.\", \"next\", \"week\", \"!\"]\n",
    "     ```\n",
    "   - **Use Case:** General-purpose word tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **`TreebankWordTokenizer`**\n",
    "   - **Implementation:** Uses the Penn Treebank tokenizer rules (same as `word_tokenize`).\n",
    "   - **Features:** Splits contractions (e.g., \"can't\" â†’ [\"ca\", \"n't\"]), handles punctuation.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import TreebankWordTokenizer\n",
    "     tokenizer = TreebankWordTokenizer()\n",
    "     print(tokenizer.tokenize(\"Can't won't don't\"))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     ['Ca', \"n't\", 'wo', \"n't\", 'do', \"n't\"]\n",
    "     ```\n",
    "   - **Use Case:** When working with text where contractions need to be split.\n",
    "\n",
    "A **contraction** is a shortened form of one or more words where missing letters are replaced by an apostrophe (`'`). Contractions are commonly used in informal writing and speech.  \n",
    "\n",
    "##### **Examples of Contractions:**\n",
    "| Full Form | Contraction |\n",
    "|-----------|------------|\n",
    "| I am | I'm |\n",
    "| You are | You're |\n",
    "| He is / He has | He's |\n",
    "| They are | They're |\n",
    "| Cannot | Can't |\n",
    "| Will not | Won't |\n",
    "| Do not | Don't |\n",
    "| Should not | Shouldn't |\n",
    "| Would have | Would've |\n",
    "\n",
    "##### **Why Do Contractions Matter in NLP?**\n",
    "- Some tokenizers **split contractions** into separate words (`\"can't\"` â†’ `[\"ca\", \"n't\"]`).\n",
    "- Others **keep contractions intact** (`\"can't\"` â†’ `[\"can't\"]`).\n",
    "- Handling contractions correctly is important for **sentiment analysis**, **text preprocessing**, and **machine learning models**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### 3. **`WordPunctTokenizer`**\n",
    "   - **Implementation:** Splits words and punctuation separately.\n",
    "   - **Features:** Breaks contractions into separate parts (e.g., \"can't\" â†’ [\"can\", \"'t\"]).\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import WordPunctTokenizer\n",
    "     tokenizer = WordPunctTokenizer()\n",
    "     print(tokenizer.tokenize(\"I'm excited!\"))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     ['I', \"'\", 'm', 'excited', '!']\n",
    "     ```\n",
    "   - **Use Case:** When punctuation needs to be treated as separate tokens.\n",
    "\n",
    "---\n",
    "There are a couple other tokenizers.\n",
    "\n",
    "##### **Summary**\n",
    "| Tokenizer | Handles Contractions? | Splits Punctuation? | Use Case |\n",
    "|-----------|------------------|----------------|---------|\n",
    "| `word_tokenize` | Yes | Mostly | General NLP tasks |\n",
    "| `TreebankWordTokenizer` | Yes (splits aggressively) | Yes | Similar to `word_tokenize`, more aggressive |\n",
    "| `ToktokTokenizer` | No | Yes | Fast and simple tokenization |\n",
    "| `RegexpTokenizer` | Custom | Custom | Custom rules-based tokenization |\n",
    "| `WordPunctTokenizer` | Yes (aggressive) | Yes (separates all punctuation) | When punctuation should be separate |\n",
    "| `MWETokenizer` | No | No | Preserve multi-word expressions |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** is the process of reducing a word to its root form (also called a \"stem\") by **removing suffixes**. It helps normalize words so that variations of a word are treated as the same.  \n",
    "\n",
    "For example:  \n",
    "- **\"running\" â†’ \"run\"**  \n",
    "- **\"flies\" â†’ \"fli\"** (incorrect but common with some stemmers)  \n",
    "- **\"happily\" â†’ \"happili\"**  \n",
    "\n",
    "Stemming is a **rule-based** approach and doesn't always produce real words. It just chops off endings based on predefined rules.\n",
    "\n",
    "---\n",
    "\n",
    "**Limitations of Stemming**\n",
    "- Doesn't always produce real words (e.g., *\"flies\"* â†’ *\"fli\"*)  \n",
    "- Different words may map to the same stem incorrectly  \n",
    "- Over-stemming (too aggressive) or under-stemming (not aggressive enough)  \n",
    "\n",
    "---\n",
    "\n",
    " **Stemming vs Lemmatization**\n",
    "If you need more **accurate** root words, **lemmatization** is a better choice because it uses a **dictionary-based** approach instead of just chopping off suffixes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating â†’ eat\n",
      "eats â†’ eat\n",
      "eaten â†’ eaten\n",
      "writing â†’ write\n",
      "writes â†’ write\n",
      "programming â†’ program\n",
      "programs â†’ program\n",
      "history â†’ histori\n",
      "finally â†’ final\n",
      "finalized â†’ final\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "for word in words:\n",
    "    print(word, \"â†’\", porter_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change from \"history\" --> \"histori\" exhibits the major disadvantage of Stemming. The meaning of the original word has changed completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "porter_stemmer.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a word. The original word lost its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runn\n",
      "happi\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# Regexp Stemming\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Define a regex pattern to remove common suffixes (-ing, -ed, -ly)\n",
    "regexp_stemmer = RegexpStemmer(r'ing$|ed$|ly$', min=4)\n",
    "\n",
    "print(regexp_stemmer.stem(\"running\"))   # Output: runn\n",
    "print(regexp_stemmer.stem(\"happily\"))   # Output: happi\n",
    "print(regexp_stemmer.stem(\"worked\"))    # Output: work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "ingeat\n"
     ]
    }
   ],
   "source": [
    "regexp_stemmer = RegexpStemmer('ing|ed$|ly$')\n",
    "print(regexp_stemmer.stem(\"ingeating\")) #eat\n",
    "regexp_stemmer = RegexpStemmer('ing$|ed$|ly$')\n",
    "print(regexp_stemmer.stem(\"ingeating\")) #ingeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happili\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "print(snowball_stemmer.stem(\"happily\"))\n",
    "print(snowball_stemmer.stem(\"worked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating â†’ eat\n",
      "eats â†’ eat\n",
      "eaten â†’ eaten\n",
      "writing â†’ write\n",
      "writes â†’ write\n",
      "programming â†’ program\n",
      "programs â†’ program\n",
      "history â†’ histori\n",
      "finally â†’ final\n",
      "finalized â†’ final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"â†’\", snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing all three stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem(\"fairly\"), porter_stemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sporting')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_stemmer.stem(\"fairly\"), regexp_stemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem(\"fairly\"), snowball_stemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1ï¸âƒ£ Porter Stemmer**\n",
    "\n",
    "âœ… **Pros:**  \n",
    "- One of the most widely used stemming algorithms.  \n",
    "- Uses a **set of heuristic rules** to remove common suffixes.  \n",
    "- Efficient and relatively fast.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "- Sometimes over-stems words (e.g., `\"flies\"` â†’ `\"fli\"`).  \n",
    "- Does not always produce real words.  \n",
    "- **Not customizable** (fixed rules).  \n",
    "\n",
    "---\n",
    "\n",
    "**2ï¸âƒ£ Regex Stemmer**\n",
    "\n",
    "âœ… **Pros:**  \n",
    "- **Customizable**: You define the regex pattern to remove suffixes.  \n",
    "- Useful for **domain-specific** text processing.  \n",
    "- Can prevent over-stemming by **setting minimum word length** (`min=` parameter).  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "- Requires **manual regex tuning** for different datasets.  \n",
    "- May **miss irregular word forms** (e.g., `\"better\"` wonâ€™t stem to `\"good\"`).  \n",
    "- **Not language-aware** (simply removes predefined suffixes).  \n",
    "---\n",
    "\n",
    "**3ï¸âƒ£ Snowball Stemmer**\n",
    "\n",
    "âœ… **Pros:**  \n",
    "- **Improved version of Porter Stemmer**.  \n",
    "- Supports **multiple languages** (e.g., English, French, Spanish).  \n",
    "- More **accurate and flexible** than Porter.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "- **Slower than Porter Stemmer** due to additional rules.  \n",
    "- Not as customizable as Regex Stemmer.  \n",
    "\n",
    "---\n",
    "\n",
    "**Comparison Table**\n",
    "\n",
    "| Feature          | **Porter Stemmer** | **Regex Stemmer** | **Snowball Stemmer** |\n",
    "|-----------------|------------------|-----------------|------------------|\n",
    "| **Algorithm**   | Rule-based       | Regex-based    | Rule-based (Improved Porter) |\n",
    "| **Customizable?** | âŒ No | âœ… Yes | âŒ No |\n",
    "| **Language Support** | âŒ English Only | âŒ Manual | âœ… Multiple Languages |\n",
    "| **Speed** | âœ… Fast | âœ… Fast | âŒ Slower (More Rules) |\n",
    "| **Accuracy** | âŒ Can over-stem | âœ… Depends on Regex | âœ… More accurate than Porter |\n",
    "| **Best Use Case** | General NLP tasks | Domain-specific text | Multi-language support |\n",
    "\n",
    "---\n",
    "\n",
    "**Which One Should You Use?**\n",
    "\n",
    "ğŸ”¹ **Use Porter Stemmer** â†’ If you want a simple, fast stemming method for **English text**.  \n",
    "ğŸ”¹ **Use Regex Stemmer** â†’ If you need **full control** over stemming rules for **custom datasets**.  \n",
    "ğŸ”¹ **Use Snowball Stemmer** â†’ If you want **better accuracy** and support for **multiple languages**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** is the process of reducing a word to its **base or dictionary form (lemma)** while ensuring it remains a real word. Unlike **stemming**, which just chops off suffixes, lemmatization considers **grammatical meaning** using a **lexical database** like WordNet.\n",
    "\n",
    "---\n",
    "\n",
    "**How Does Lemmatization Work?**\n",
    "\n",
    "- **Considers the context** and part of speech (POS) of a word.  \n",
    "- Uses a **dictionary lookup** to find the root form (lemma).  \n",
    "- Ensures the output is a valid word.  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "| Word | Stemmed (Porter) | Lemmatized (WordNet) |\n",
    "|------|----------------|------------------|\n",
    "| Running | run | run |\n",
    "| Better | better | good |\n",
    "| Studies | studi | study |\n",
    "| Mice | mice | mouse |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ”¹ **Explanation:**  \n",
    "\n",
    "- `\"running\"` (verb) â†’ `\"run\"` âœ…  \n",
    "- `\"better\"` (adjective) â†’ `\"good\"` âœ…  \n",
    "- `\"mice\"` (noun) â†’ `\"mouse\"` âœ…  \n",
    "- `\"studies\"` (noun) â†’ `\"study\"` âœ…  \n",
    "\n",
    "ğŸš¨ **POS Tagging is Important!**  \n",
    "\n",
    "- If no `pos` is provided, it assumes the word is a **noun**.  \n",
    "- `\"running\"` â†’ `\"running\"` (incorrect)  \n",
    "- `\"running\", pos=\"v\"` â†’ `\"run\"` (correct)  \n",
    "\n",
    "---\n",
    "\n",
    "**Stemming vs Lemmatization**\n",
    "\n",
    "| Feature | **Stemming** | **Lemmatization** |\n",
    "|---------|-------------|------------------|\n",
    "| **Method** | Removes suffixes | Uses dictionary lookup |\n",
    "| **Grammar Aware?** | âŒ No | âœ… Yes |\n",
    "| **Produces Real Words?** | âŒ No | âœ… Yes |\n",
    "| **Computational Cost** | âœ… Fast | âŒ Slower |\n",
    "| **Example (\"better\")** | **\"better\"** â†’ **\"better\"** | **\"better\"** â†’ **\"good\"** |\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use Lemmatization?**\n",
    "\n",
    "âœ… **Linguistic Accuracy Required** (e.g., chatbots, search engines).  \n",
    "âœ… **When Meaning Matters** (e.g., sentiment analysis).  \n",
    "âœ… **If You Have Computational Resources** (since it's slower than stemming).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating â†’ eating\n",
      "eats â†’ eats\n",
      "eaten â†’ eaten\n",
      "writing â†’ writing\n",
      "writes â†’ writes\n",
      "programming â†’ programming\n",
      "programs â†’ program\n",
      "history â†’ history\n",
      "finally â†’ finally\n",
      "finalized â†’ finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"â†’\", lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes as all these words exist in the dictionary. Now we will add a \"pos\" attribute in the lemmatize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating â†’ eat\n",
      "eats â†’ eat\n",
      "eaten â†’ eat\n",
      "writing â†’ write\n",
      "writes â†’ write\n",
      "programming â†’ program\n",
      "programs â†’ program\n",
      "history â†’ history\n",
      "finally â†’ finally\n",
      "finalized â†’ finalize\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Pos Tagging\n",
    "Noun- n\n",
    "Verb- v\n",
    "Adjective- a\n",
    "Adverb- r\n",
    "'''\n",
    "# by default it's noun\n",
    "for word in words:\n",
    "    print(word, \"â†’\", lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating â†’ eating\n",
      "eats â†’ eats\n",
      "eaten â†’ eaten\n",
      "writing â†’ writing\n",
      "writes â†’ writes\n",
      "programming â†’ programming\n",
      "programs â†’ programs\n",
      "history â†’ history\n",
      "finally â†’ finally\n",
      "finalized â†’ finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word, \"â†’\", lemmatizer.lemmatize(word, pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\", pos='a'), lemmatizer.lemmatize(\"sportingly\", pos = 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage of Lemmatization over Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem(\"better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"better\", 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are **common words** in a language that are **filtered out** in Natural Language Processing (NLP) because they **do not carry much meaning**. These include words like *\"the,\" \"is,\" \"and,\" \"in,\" \"of,\"* etc.  \n",
    "\n",
    "Since stopwords appear **frequently** in text but add little value to analysis, they are often **removed** to improve efficiency in NLP tasks like text classification, search engines, and machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples of Stopwords in English** \n",
    " \n",
    "| Category | Examples |\n",
    "|----------|----------|\n",
    "| Articles | the, a, an |\n",
    "| Pronouns | he, she, it, they |\n",
    "| Conjunctions | and, or, but, nor |\n",
    "| Prepositions | in, on, at, with |\n",
    "| Auxiliary Verbs | is, are, was, were, be |\n",
    "| Other Common Words | this, that, those, these |\n",
    "\n",
    "---\n",
    "\n",
    "**Stopwords in NLTK**\n",
    "\n",
    "NLTK provides a built-in list of stopwords for multiple languages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for Indiaâ€™s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isnâ€™t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english') # will print stopwords of English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_stopwords) # number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'mÃ¡s',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sÃ­',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'tambiÃ©n',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mÃ­',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'quÃ©',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'Ã©l',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tÃº',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mÃ­o',\n",
       " 'mÃ­a',\n",
       " 'mÃ­os',\n",
       " 'mÃ­as',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estÃ¡s',\n",
       " 'estÃ¡',\n",
       " 'estamos',\n",
       " 'estÃ¡is',\n",
       " 'estÃ¡n',\n",
       " 'estÃ©',\n",
       " 'estÃ©s',\n",
       " 'estemos',\n",
       " 'estÃ©is',\n",
       " 'estÃ©n',\n",
       " 'estarÃ©',\n",
       " 'estarÃ¡s',\n",
       " 'estarÃ¡',\n",
       " 'estaremos',\n",
       " 'estarÃ©is',\n",
       " 'estarÃ¡n',\n",
       " 'estarÃ­a',\n",
       " 'estarÃ­as',\n",
       " 'estarÃ­amos',\n",
       " 'estarÃ­ais',\n",
       " 'estarÃ­an',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estÃ¡bamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviÃ©ramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviÃ©semos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habÃ©is',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayÃ¡is',\n",
       " 'hayan',\n",
       " 'habrÃ©',\n",
       " 'habrÃ¡s',\n",
       " 'habrÃ¡',\n",
       " 'habremos',\n",
       " 'habrÃ©is',\n",
       " 'habrÃ¡n',\n",
       " 'habrÃ­a',\n",
       " 'habrÃ­as',\n",
       " 'habrÃ­amos',\n",
       " 'habrÃ­ais',\n",
       " 'habrÃ­an',\n",
       " 'habÃ­a',\n",
       " 'habÃ­as',\n",
       " 'habÃ­amos',\n",
       " 'habÃ­ais',\n",
       " 'habÃ­an',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiÃ©ramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiÃ©semos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seÃ¡is',\n",
       " 'sean',\n",
       " 'serÃ©',\n",
       " 'serÃ¡s',\n",
       " 'serÃ¡',\n",
       " 'seremos',\n",
       " 'serÃ©is',\n",
       " 'serÃ¡n',\n",
       " 'serÃ­a',\n",
       " 'serÃ­as',\n",
       " 'serÃ­amos',\n",
       " 'serÃ­ais',\n",
       " 'serÃ­an',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'Ã©ramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuÃ©ramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuÃ©semos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenÃ©is',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengÃ¡is',\n",
       " 'tengan',\n",
       " 'tendrÃ©',\n",
       " 'tendrÃ¡s',\n",
       " 'tendrÃ¡',\n",
       " 'tendremos',\n",
       " 'tendrÃ©is',\n",
       " 'tendrÃ¡n',\n",
       " 'tendrÃ­a',\n",
       " 'tendrÃ­as',\n",
       " 'tendrÃ­amos',\n",
       " 'tendrÃ­ais',\n",
       " 'tendrÃ­an',\n",
       " 'tenÃ­a',\n",
       " 'tenÃ­as',\n",
       " 'tenÃ­amos',\n",
       " 'tenÃ­ais',\n",
       " 'tenÃ­an',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviÃ©ramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviÃ©semos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('spanish') # will print stopwords of Spanish Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø¥Ø°',\n",
       " 'Ø¥Ø°Ø§',\n",
       " 'Ø¥Ø°Ù…Ø§',\n",
       " 'Ø¥Ø°Ù†',\n",
       " 'Ø£Ù',\n",
       " 'Ø£Ù‚Ù„',\n",
       " 'Ø£ÙƒØ«Ø±',\n",
       " 'Ø£Ù„Ø§',\n",
       " 'Ø¥Ù„Ø§',\n",
       " 'Ø§Ù„ØªÙŠ',\n",
       " 'Ø§Ù„Ø°ÙŠ',\n",
       " 'Ø§Ù„Ø°ÙŠÙ†',\n",
       " 'Ø§Ù„Ù„Ø§ØªÙŠ',\n",
       " 'Ø§Ù„Ù„Ø§Ø¦ÙŠ',\n",
       " 'Ø§Ù„Ù„ØªØ§Ù†',\n",
       " 'Ø§Ù„Ù„ØªÙŠØ§',\n",
       " 'Ø§Ù„Ù„ØªÙŠÙ†',\n",
       " 'Ø§Ù„Ù„Ø°Ø§Ù†',\n",
       " 'Ø§Ù„Ù„Ø°ÙŠÙ†',\n",
       " 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ',\n",
       " 'Ø¥Ù„Ù‰',\n",
       " 'Ø¥Ù„ÙŠÙƒ',\n",
       " 'Ø¥Ù„ÙŠÙƒÙ…',\n",
       " 'Ø¥Ù„ÙŠÙƒÙ…Ø§',\n",
       " 'Ø¥Ù„ÙŠÙƒÙ†',\n",
       " 'Ø£Ù…',\n",
       " 'Ø£Ù…Ø§',\n",
       " 'Ø£Ù…Ø§',\n",
       " 'Ø¥Ù…Ø§',\n",
       " 'Ø£Ù†',\n",
       " 'Ø¥Ù†',\n",
       " 'Ø¥Ù†Ø§',\n",
       " 'Ø£Ù†Ø§',\n",
       " 'Ø£Ù†Øª',\n",
       " 'Ø£Ù†ØªÙ…',\n",
       " 'Ø£Ù†ØªÙ…Ø§',\n",
       " 'Ø£Ù†ØªÙ†',\n",
       " 'Ø¥Ù†Ù…Ø§',\n",
       " 'Ø¥Ù†Ù‡',\n",
       " 'Ø£Ù†Ù‰',\n",
       " 'Ø£Ù†Ù‰',\n",
       " 'Ø¢Ù‡',\n",
       " 'Ø¢Ù‡Ø§',\n",
       " 'Ø£Ùˆ',\n",
       " 'Ø£ÙˆÙ„Ø§Ø¡',\n",
       " 'Ø£ÙˆÙ„Ø¦Ùƒ',\n",
       " 'Ø£ÙˆÙ‡',\n",
       " 'Ø¢ÙŠ',\n",
       " 'Ø£ÙŠ',\n",
       " 'Ø£ÙŠÙ‡Ø§',\n",
       " 'Ø¥ÙŠ',\n",
       " 'Ø£ÙŠÙ†',\n",
       " 'Ø£ÙŠÙ†',\n",
       " 'Ø£ÙŠÙ†Ù…Ø§',\n",
       " 'Ø¥ÙŠÙ‡',\n",
       " 'Ø¨Ø®',\n",
       " 'Ø¨Ø³',\n",
       " 'Ø¨Ø¹Ø¯',\n",
       " 'Ø¨Ø¹Ø¶',\n",
       " 'Ø¨Ùƒ',\n",
       " 'Ø¨ÙƒÙ…',\n",
       " 'Ø¨ÙƒÙ…',\n",
       " 'Ø¨ÙƒÙ…Ø§',\n",
       " 'Ø¨ÙƒÙ†',\n",
       " 'Ø¨Ù„',\n",
       " 'Ø¨Ù„Ù‰',\n",
       " 'Ø¨Ù…Ø§',\n",
       " 'Ø¨Ù…Ø§Ø°Ø§',\n",
       " 'Ø¨Ù…Ù†',\n",
       " 'Ø¨Ù†Ø§',\n",
       " 'Ø¨Ù‡',\n",
       " 'Ø¨Ù‡Ø§',\n",
       " 'Ø¨Ù‡Ù…',\n",
       " 'Ø¨Ù‡Ù…Ø§',\n",
       " 'Ø¨Ù‡Ù†',\n",
       " 'Ø¨ÙŠ',\n",
       " 'Ø¨ÙŠÙ†',\n",
       " 'Ø¨ÙŠØ¯',\n",
       " 'ØªÙ„Ùƒ',\n",
       " 'ØªÙ„ÙƒÙ…',\n",
       " 'ØªÙ„ÙƒÙ…Ø§',\n",
       " 'ØªÙ‡',\n",
       " 'ØªÙŠ',\n",
       " 'ØªÙŠÙ†',\n",
       " 'ØªÙŠÙ†Ùƒ',\n",
       " 'Ø«Ù…',\n",
       " 'Ø«Ù…Ø©',\n",
       " 'Ø­Ø§Ø´Ø§',\n",
       " 'Ø­Ø¨Ø°Ø§',\n",
       " 'Ø­ØªÙ‰',\n",
       " 'Ø­ÙŠØ«',\n",
       " 'Ø­ÙŠØ«Ù…Ø§',\n",
       " 'Ø­ÙŠÙ†',\n",
       " 'Ø®Ù„Ø§',\n",
       " 'Ø¯ÙˆÙ†',\n",
       " 'Ø°Ø§',\n",
       " 'Ø°Ø§Øª',\n",
       " 'Ø°Ø§Ùƒ',\n",
       " 'Ø°Ø§Ù†',\n",
       " 'Ø°Ø§Ù†Ùƒ',\n",
       " 'Ø°Ù„Ùƒ',\n",
       " 'Ø°Ù„ÙƒÙ…',\n",
       " 'Ø°Ù„ÙƒÙ…Ø§',\n",
       " 'Ø°Ù„ÙƒÙ†',\n",
       " 'Ø°Ù‡',\n",
       " 'Ø°Ùˆ',\n",
       " 'Ø°ÙˆØ§',\n",
       " 'Ø°ÙˆØ§ØªØ§',\n",
       " 'Ø°ÙˆØ§ØªÙŠ',\n",
       " 'Ø°ÙŠ',\n",
       " 'Ø°ÙŠÙ†',\n",
       " 'Ø°ÙŠÙ†Ùƒ',\n",
       " 'Ø±ÙŠØ«',\n",
       " 'Ø³ÙˆÙ',\n",
       " 'Ø³ÙˆÙ‰',\n",
       " 'Ø´ØªØ§Ù†',\n",
       " 'Ø¹Ø¯Ø§',\n",
       " 'Ø¹Ø³Ù‰',\n",
       " 'Ø¹Ù„',\n",
       " 'Ø¹Ù„Ù‰',\n",
       " 'Ø¹Ù„ÙŠÙƒ',\n",
       " 'Ø¹Ù„ÙŠÙ‡',\n",
       " 'Ø¹Ù…Ø§',\n",
       " 'Ø¹Ù†',\n",
       " 'Ø¹Ù†Ø¯',\n",
       " 'ØºÙŠØ±',\n",
       " 'ÙØ¥Ø°Ø§',\n",
       " 'ÙØ¥Ù†',\n",
       " 'ÙÙ„Ø§',\n",
       " 'ÙÙ…Ù†',\n",
       " 'ÙÙŠ',\n",
       " 'ÙÙŠÙ…',\n",
       " 'ÙÙŠÙ…Ø§',\n",
       " 'ÙÙŠÙ‡',\n",
       " 'ÙÙŠÙ‡Ø§',\n",
       " 'Ù‚Ø¯',\n",
       " 'ÙƒØ£Ù†',\n",
       " 'ÙƒØ£Ù†Ù…Ø§',\n",
       " 'ÙƒØ£ÙŠ',\n",
       " 'ÙƒØ£ÙŠÙ†',\n",
       " 'ÙƒØ°Ø§',\n",
       " 'ÙƒØ°Ù„Ùƒ',\n",
       " 'ÙƒÙ„',\n",
       " 'ÙƒÙ„Ø§',\n",
       " 'ÙƒÙ„Ø§Ù‡Ù…Ø§',\n",
       " 'ÙƒÙ„ØªØ§',\n",
       " 'ÙƒÙ„Ù…Ø§',\n",
       " 'ÙƒÙ„ÙŠÙƒÙ…Ø§',\n",
       " 'ÙƒÙ„ÙŠÙ‡Ù…Ø§',\n",
       " 'ÙƒÙ…',\n",
       " 'ÙƒÙ…',\n",
       " 'ÙƒÙ…Ø§',\n",
       " 'ÙƒÙŠ',\n",
       " 'ÙƒÙŠØª',\n",
       " 'ÙƒÙŠÙ',\n",
       " 'ÙƒÙŠÙÙ…Ø§',\n",
       " 'Ù„Ø§',\n",
       " 'Ù„Ø§Ø³ÙŠÙ…Ø§',\n",
       " 'Ù„Ø¯Ù‰',\n",
       " 'Ù„Ø³Øª',\n",
       " 'Ù„Ø³ØªÙ…',\n",
       " 'Ù„Ø³ØªÙ…Ø§',\n",
       " 'Ù„Ø³ØªÙ†',\n",
       " 'Ù„Ø³Ù†',\n",
       " 'Ù„Ø³Ù†Ø§',\n",
       " 'Ù„Ø¹Ù„',\n",
       " 'Ù„Ùƒ',\n",
       " 'Ù„ÙƒÙ…',\n",
       " 'Ù„ÙƒÙ…Ø§',\n",
       " 'Ù„ÙƒÙ†',\n",
       " 'Ù„ÙƒÙ†Ù…Ø§',\n",
       " 'Ù„ÙƒÙŠ',\n",
       " 'Ù„ÙƒÙŠÙ„Ø§',\n",
       " 'Ù„Ù…',\n",
       " 'Ù„Ù…Ø§',\n",
       " 'Ù„Ù†',\n",
       " 'Ù„Ù†Ø§',\n",
       " 'Ù„Ù‡',\n",
       " 'Ù„Ù‡Ø§',\n",
       " 'Ù„Ù‡Ù…',\n",
       " 'Ù„Ù‡Ù…Ø§',\n",
       " 'Ù„Ù‡Ù†',\n",
       " 'Ù„Ùˆ',\n",
       " 'Ù„ÙˆÙ„Ø§',\n",
       " 'Ù„ÙˆÙ…Ø§',\n",
       " 'Ù„ÙŠ',\n",
       " 'Ù„Ø¦Ù†',\n",
       " 'Ù„ÙŠØª',\n",
       " 'Ù„ÙŠØ³',\n",
       " 'Ù„ÙŠØ³Ø§',\n",
       " 'Ù„ÙŠØ³Øª',\n",
       " 'Ù„ÙŠØ³ØªØ§',\n",
       " 'Ù„ÙŠØ³ÙˆØ§',\n",
       " 'Ù…Ø§',\n",
       " 'Ù…Ø§Ø°Ø§',\n",
       " 'Ù…ØªÙ‰',\n",
       " 'Ù…Ø°',\n",
       " 'Ù…Ø¹',\n",
       " 'Ù…Ù…Ø§',\n",
       " 'Ù…Ù…Ù†',\n",
       " 'Ù…Ù†',\n",
       " 'Ù…Ù†Ù‡',\n",
       " 'Ù…Ù†Ù‡Ø§',\n",
       " 'Ù…Ù†Ø°',\n",
       " 'Ù…Ù‡',\n",
       " 'Ù…Ù‡Ù…Ø§',\n",
       " 'Ù†Ø­Ù†',\n",
       " 'Ù†Ø­Ùˆ',\n",
       " 'Ù†Ø¹Ù…',\n",
       " 'Ù‡Ø§',\n",
       " 'Ù‡Ø§ØªØ§Ù†',\n",
       " 'Ù‡Ø§ØªÙ‡',\n",
       " 'Ù‡Ø§ØªÙŠ',\n",
       " 'Ù‡Ø§ØªÙŠÙ†',\n",
       " 'Ù‡Ø§Ùƒ',\n",
       " 'Ù‡Ø§Ù‡Ù†Ø§',\n",
       " 'Ù‡Ø°Ø§',\n",
       " 'Ù‡Ø°Ø§Ù†',\n",
       " 'Ù‡Ø°Ù‡',\n",
       " 'Ù‡Ø°ÙŠ',\n",
       " 'Ù‡Ø°ÙŠÙ†',\n",
       " 'Ù‡ÙƒØ°Ø§',\n",
       " 'Ù‡Ù„',\n",
       " 'Ù‡Ù„Ø§',\n",
       " 'Ù‡Ù…',\n",
       " 'Ù‡Ù…Ø§',\n",
       " 'Ù‡Ù†',\n",
       " 'Ù‡Ù†Ø§',\n",
       " 'Ù‡Ù†Ø§Ùƒ',\n",
       " 'Ù‡Ù†Ø§Ù„Ùƒ',\n",
       " 'Ù‡Ùˆ',\n",
       " 'Ù‡Ø¤Ù„Ø§Ø¡',\n",
       " 'Ù‡ÙŠ',\n",
       " 'Ù‡ÙŠØ§',\n",
       " 'Ù‡ÙŠØª',\n",
       " 'Ù‡ÙŠÙ‡Ø§Øª',\n",
       " 'ÙˆØ§Ù„Ø°ÙŠ',\n",
       " 'ÙˆØ§Ù„Ø°ÙŠÙ†',\n",
       " 'ÙˆØ¥Ø°',\n",
       " 'ÙˆØ¥Ø°Ø§',\n",
       " 'ÙˆØ¥Ù†',\n",
       " 'ÙˆÙ„Ø§',\n",
       " 'ÙˆÙ„ÙƒÙ†',\n",
       " 'ÙˆÙ„Ùˆ',\n",
       " 'ÙˆÙ…Ø§',\n",
       " 'ÙˆÙ…Ù†',\n",
       " 'ÙˆÙ‡Ùˆ',\n",
       " 'ÙŠØ§',\n",
       " 'Ø£Ø¨ÙŒ',\n",
       " 'Ø£Ø®ÙŒ',\n",
       " 'Ø­Ù…ÙŒ',\n",
       " 'ÙÙˆ',\n",
       " 'Ø£Ù†ØªÙ',\n",
       " 'ÙŠÙ†Ø§ÙŠØ±',\n",
       " 'ÙØ¨Ø±Ø§ÙŠØ±',\n",
       " 'Ù…Ø§Ø±Ø³',\n",
       " 'Ø£Ø¨Ø±ÙŠÙ„',\n",
       " 'Ù…Ø§ÙŠÙˆ',\n",
       " 'ÙŠÙˆÙ†ÙŠÙˆ',\n",
       " 'ÙŠÙˆÙ„ÙŠÙˆ',\n",
       " 'Ø£ØºØ³Ø·Ø³',\n",
       " 'Ø³Ø¨ØªÙ…Ø¨Ø±',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ù†ÙˆÙÙ…Ø¨Ø±',\n",
       " 'Ø¯ÙŠØ³Ù…Ø¨Ø±',\n",
       " 'Ø¬Ø§Ù†ÙÙŠ',\n",
       " 'ÙÙŠÙØ±ÙŠ',\n",
       " 'Ù…Ø§Ø±Ø³',\n",
       " 'Ø£ÙØ±ÙŠÙ„',\n",
       " 'Ù…Ø§ÙŠ',\n",
       " 'Ø¬ÙˆØ§Ù†',\n",
       " 'Ø¬ÙˆÙŠÙ„ÙŠØ©',\n",
       " 'Ø£ÙˆØª',\n",
       " 'ÙƒØ§Ù†ÙˆÙ†',\n",
       " 'Ø´Ø¨Ø§Ø·',\n",
       " 'Ø¢Ø°Ø§Ø±',\n",
       " 'Ù†ÙŠØ³Ø§Ù†',\n",
       " 'Ø£ÙŠØ§Ø±',\n",
       " 'Ø­Ø²ÙŠØ±Ø§Ù†',\n",
       " 'ØªÙ…ÙˆØ²',\n",
       " 'Ø¢Ø¨',\n",
       " 'Ø£ÙŠÙ„ÙˆÙ„',\n",
       " 'ØªØ´Ø±ÙŠÙ†',\n",
       " 'Ø¯ÙˆÙ„Ø§Ø±',\n",
       " 'Ø¯ÙŠÙ†Ø§Ø±',\n",
       " 'Ø±ÙŠØ§Ù„',\n",
       " 'Ø¯Ø±Ù‡Ù…',\n",
       " 'Ù„ÙŠØ±Ø©',\n",
       " 'Ø¬Ù†ÙŠÙ‡',\n",
       " 'Ù‚Ø±Ø´',\n",
       " 'Ù…Ù„ÙŠÙ…',\n",
       " 'ÙÙ„Ø³',\n",
       " 'Ù‡Ù„Ù„Ø©',\n",
       " 'Ø³Ù†ØªÙŠÙ…',\n",
       " 'ÙŠÙˆØ±Ùˆ',\n",
       " 'ÙŠÙ†',\n",
       " 'ÙŠÙˆØ§Ù†',\n",
       " 'Ø´ÙŠÙƒÙ„',\n",
       " 'ÙˆØ§Ø­Ø¯',\n",
       " 'Ø§Ø«Ù†Ø§Ù†',\n",
       " 'Ø«Ù„Ø§Ø«Ø©',\n",
       " 'Ø£Ø±Ø¨Ø¹Ø©',\n",
       " 'Ø®Ù…Ø³Ø©',\n",
       " 'Ø³ØªØ©',\n",
       " 'Ø³Ø¨Ø¹Ø©',\n",
       " 'Ø«Ù…Ø§Ù†ÙŠØ©',\n",
       " 'ØªØ³Ø¹Ø©',\n",
       " 'Ø¹Ø´Ø±Ø©',\n",
       " 'Ø£Ø­Ø¯',\n",
       " 'Ø§Ø«Ù†Ø§',\n",
       " 'Ø§Ø«Ù†ÙŠ',\n",
       " 'Ø¥Ø­Ø¯Ù‰',\n",
       " 'Ø«Ù„Ø§Ø«',\n",
       " 'Ø£Ø±Ø¨Ø¹',\n",
       " 'Ø®Ù…Ø³',\n",
       " 'Ø³Øª',\n",
       " 'Ø³Ø¨Ø¹',\n",
       " 'Ø«Ù…Ø§Ù†ÙŠ',\n",
       " 'ØªØ³Ø¹',\n",
       " 'Ø¹Ø´Ø±',\n",
       " 'Ø«Ù…Ø§Ù†',\n",
       " 'Ø³Ø¨Øª',\n",
       " 'Ø£Ø­Ø¯',\n",
       " 'Ø§Ø«Ù†ÙŠÙ†',\n",
       " 'Ø«Ù„Ø§Ø«Ø§Ø¡',\n",
       " 'Ø£Ø±Ø¨Ø¹Ø§Ø¡',\n",
       " 'Ø®Ù…ÙŠØ³',\n",
       " 'Ø¬Ù…Ø¹Ø©',\n",
       " 'Ø£ÙˆÙ„',\n",
       " 'Ø«Ø§Ù†',\n",
       " 'Ø«Ø§Ù†ÙŠ',\n",
       " 'Ø«Ø§Ù„Ø«',\n",
       " 'Ø±Ø§Ø¨Ø¹',\n",
       " 'Ø®Ø§Ù…Ø³',\n",
       " 'Ø³Ø§Ø¯Ø³',\n",
       " 'Ø³Ø§Ø¨Ø¹',\n",
       " 'Ø«Ø§Ù…Ù†',\n",
       " 'ØªØ§Ø³Ø¹',\n",
       " 'Ø¹Ø§Ø´Ø±',\n",
       " 'Ø­Ø§Ø¯ÙŠ',\n",
       " 'Ø£',\n",
       " 'Ø¨',\n",
       " 'Øª',\n",
       " 'Ø«',\n",
       " 'Ø¬',\n",
       " 'Ø­',\n",
       " 'Ø®',\n",
       " 'Ø¯',\n",
       " 'Ø°',\n",
       " 'Ø±',\n",
       " 'Ø²',\n",
       " 'Ø³',\n",
       " 'Ø´',\n",
       " 'Øµ',\n",
       " 'Ø¶',\n",
       " 'Ø·',\n",
       " 'Ø¸',\n",
       " 'Ø¹',\n",
       " 'Øº',\n",
       " 'Ù',\n",
       " 'Ù‚',\n",
       " 'Ùƒ',\n",
       " 'Ù„',\n",
       " 'Ù…',\n",
       " 'Ù†',\n",
       " 'Ù‡',\n",
       " 'Ùˆ',\n",
       " 'ÙŠ',\n",
       " 'Ø¡',\n",
       " 'Ù‰',\n",
       " 'Ø¢',\n",
       " 'Ø¤',\n",
       " 'Ø¦',\n",
       " 'Ø£',\n",
       " 'Ø©',\n",
       " 'Ø£Ù„Ù',\n",
       " 'Ø¨Ø§Ø¡',\n",
       " 'ØªØ§Ø¡',\n",
       " 'Ø«Ø§Ø¡',\n",
       " 'Ø¬ÙŠÙ…',\n",
       " 'Ø­Ø§Ø¡',\n",
       " 'Ø®Ø§Ø¡',\n",
       " 'Ø¯Ø§Ù„',\n",
       " 'Ø°Ø§Ù„',\n",
       " 'Ø±Ø§Ø¡',\n",
       " 'Ø²Ø§ÙŠ',\n",
       " 'Ø³ÙŠÙ†',\n",
       " 'Ø´ÙŠÙ†',\n",
       " 'ØµØ§Ø¯',\n",
       " 'Ø¶Ø§Ø¯',\n",
       " 'Ø·Ø§Ø¡',\n",
       " 'Ø¸Ø§Ø¡',\n",
       " 'Ø¹ÙŠÙ†',\n",
       " 'ØºÙŠÙ†',\n",
       " 'ÙØ§Ø¡',\n",
       " 'Ù‚Ø§Ù',\n",
       " 'ÙƒØ§Ù',\n",
       " 'Ù„Ø§Ù…',\n",
       " 'Ù…ÙŠÙ…',\n",
       " 'Ù†ÙˆÙ†',\n",
       " 'Ù‡Ø§Ø¡',\n",
       " 'ÙˆØ§Ùˆ',\n",
       " 'ÙŠØ§Ø¡',\n",
       " 'Ù‡Ù…Ø²Ø©',\n",
       " 'ÙŠ',\n",
       " 'Ù†Ø§',\n",
       " 'Ùƒ',\n",
       " 'ÙƒÙ†',\n",
       " 'Ù‡',\n",
       " 'Ø¥ÙŠØ§Ù‡',\n",
       " 'Ø¥ÙŠØ§Ù‡Ø§',\n",
       " 'Ø¥ÙŠØ§Ù‡Ù…Ø§',\n",
       " 'Ø¥ÙŠØ§Ù‡Ù…',\n",
       " 'Ø¥ÙŠØ§Ù‡Ù†',\n",
       " 'Ø¥ÙŠØ§Ùƒ',\n",
       " 'Ø¥ÙŠØ§ÙƒÙ…Ø§',\n",
       " 'Ø¥ÙŠØ§ÙƒÙ…',\n",
       " 'Ø¥ÙŠØ§Ùƒ',\n",
       " 'Ø¥ÙŠØ§ÙƒÙ†',\n",
       " 'Ø¥ÙŠØ§ÙŠ',\n",
       " 'Ø¥ÙŠØ§Ù†Ø§',\n",
       " 'Ø£ÙˆÙ„Ø§Ù„Ùƒ',\n",
       " 'ØªØ§Ù†Ù',\n",
       " 'ØªØ§Ù†ÙÙƒ',\n",
       " 'ØªÙÙ‡',\n",
       " 'ØªÙÙŠ',\n",
       " 'ØªÙÙŠÙ’Ù†Ù',\n",
       " 'Ø«Ù…Ù‘',\n",
       " 'Ø«Ù…Ù‘Ø©',\n",
       " 'Ø°Ø§Ù†Ù',\n",
       " 'Ø°ÙÙ‡',\n",
       " 'Ø°ÙÙŠ',\n",
       " 'Ø°ÙÙŠÙ’Ù†Ù',\n",
       " 'Ù‡ÙØ¤Ù„Ø§Ø¡',\n",
       " 'Ù‡ÙØ§ØªØ§Ù†Ù',\n",
       " 'Ù‡ÙØ§ØªÙÙ‡',\n",
       " 'Ù‡ÙØ§ØªÙÙŠ',\n",
       " 'Ù‡ÙØ§ØªÙÙŠÙ’Ù†Ù',\n",
       " 'Ù‡ÙØ°Ø§',\n",
       " 'Ù‡ÙØ°Ø§Ù†Ù',\n",
       " 'Ù‡ÙØ°ÙÙ‡',\n",
       " 'Ù‡ÙØ°ÙÙŠ',\n",
       " 'Ù‡ÙØ°ÙÙŠÙ’Ù†Ù',\n",
       " 'Ø§Ù„Ø£Ù„Ù‰',\n",
       " 'Ø§Ù„Ø£Ù„Ø§Ø¡',\n",
       " 'Ø£Ù„',\n",
       " 'Ø£Ù†Ù‘Ù‰',\n",
       " 'Ø£ÙŠÙ‘',\n",
       " 'Ù‘Ø£ÙŠÙ‘Ø§Ù†',\n",
       " 'Ø£Ù†Ù‘Ù‰',\n",
       " 'Ø£ÙŠÙ‘',\n",
       " 'Ù‘Ø£ÙŠÙ‘Ø§Ù†',\n",
       " 'Ø°ÙŠØª',\n",
       " 'ÙƒØ£ÙŠÙ‘',\n",
       " 'ÙƒØ£ÙŠÙ‘Ù†',\n",
       " 'Ø¨Ø¶Ø¹',\n",
       " 'ÙÙ„Ø§Ù†',\n",
       " 'ÙˆØ§',\n",
       " 'Ø¢Ù…ÙŠÙ†Ù',\n",
       " 'Ø¢Ù‡Ù',\n",
       " 'Ø¢Ù‡Ù',\n",
       " 'Ø¢Ù‡Ø§Ù‹',\n",
       " 'Ø£ÙÙÙÙ‘',\n",
       " 'Ø£ÙÙÙÙ‘',\n",
       " 'Ø£ÙÙÙ‘',\n",
       " 'Ø£Ù…Ø§Ù…Ùƒ',\n",
       " 'Ø£Ù…Ø§Ù…ÙƒÙ',\n",
       " 'Ø£ÙˆÙ‘Ù‡Ù’',\n",
       " 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ',\n",
       " 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ',\n",
       " 'Ø¥Ù„ÙŠÙƒÙ',\n",
       " 'Ø¥Ù„ÙŠÙƒÙ†Ù‘',\n",
       " 'Ø¥ÙŠÙ‡Ù',\n",
       " 'Ø¨Ø®Ù',\n",
       " 'Ø¨Ø³Ù‘',\n",
       " 'Ø¨ÙØ³Ù’',\n",
       " 'Ø¨Ø·Ø¢Ù†',\n",
       " 'Ø¨ÙÙ„Ù’Ù‡Ù',\n",
       " 'Ø­Ø§ÙŠ',\n",
       " 'Ø­ÙØ°Ø§Ø±Ù',\n",
       " 'Ø­ÙŠÙÙ‘',\n",
       " 'Ø­ÙŠÙÙ‘',\n",
       " 'Ø¯ÙˆÙ†Ùƒ',\n",
       " 'Ø±ÙˆÙŠØ¯Ùƒ',\n",
       " 'Ø³Ø±Ø¹Ø§Ù†',\n",
       " 'Ø´ØªØ§Ù†Ù',\n",
       " 'Ø´ÙØªÙÙ‘Ø§Ù†Ù',\n",
       " 'ØµÙ‡Ù’',\n",
       " 'ØµÙ‡Ù',\n",
       " 'Ø·Ø§Ù‚',\n",
       " 'Ø·ÙÙ‚',\n",
       " 'Ø¹ÙØ¯ÙØ³Ù’',\n",
       " 'ÙƒÙØ®',\n",
       " 'Ù…ÙƒØ§Ù†ÙÙƒ',\n",
       " 'Ù…ÙƒØ§Ù†ÙÙƒ',\n",
       " 'Ù…ÙƒØ§Ù†ÙÙƒ',\n",
       " 'Ù…ÙƒØ§Ù†ÙƒÙ…',\n",
       " 'Ù…ÙƒØ§Ù†ÙƒÙ…Ø§',\n",
       " 'Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘',\n",
       " 'Ù†ÙØ®Ù’',\n",
       " 'Ù‡Ø§ÙƒÙ',\n",
       " 'Ù‡ÙØ¬Ù’',\n",
       " 'Ù‡Ù„Ù…',\n",
       " 'Ù‡ÙŠÙ‘Ø§',\n",
       " 'Ù‡ÙÙŠÙ’Ù‡Ø§Øª',\n",
       " 'ÙˆØ§',\n",
       " 'ÙˆØ§Ù‡Ø§Ù‹',\n",
       " 'ÙˆØ±Ø§Ø¡ÙÙƒ',\n",
       " 'ÙˆÙØ´Ù’ÙƒÙØ§Ù†Ù',\n",
       " 'ÙˆÙÙŠÙ’',\n",
       " 'ÙŠÙØ¹Ù„Ø§Ù†',\n",
       " 'ØªÙØ¹Ù„Ø§Ù†',\n",
       " 'ÙŠÙØ¹Ù„ÙˆÙ†',\n",
       " 'ØªÙØ¹Ù„ÙˆÙ†',\n",
       " 'ØªÙØ¹Ù„ÙŠÙ†',\n",
       " 'Ø§ØªØ®Ø°',\n",
       " 'Ø£Ù„ÙÙ‰',\n",
       " 'ØªØ®Ø°',\n",
       " 'ØªØ±Ùƒ',\n",
       " 'ØªØ¹Ù„ÙÙ‘Ù…',\n",
       " 'Ø¬Ø¹Ù„',\n",
       " 'Ø­Ø¬Ø§',\n",
       " 'Ø­Ø¨ÙŠØ¨',\n",
       " 'Ø®Ø§Ù„',\n",
       " 'Ø­Ø³Ø¨',\n",
       " 'Ø®Ø§Ù„',\n",
       " 'Ø¯Ø±Ù‰',\n",
       " 'Ø±Ø£Ù‰',\n",
       " 'Ø²Ø¹Ù…',\n",
       " 'ØµØ¨Ø±',\n",
       " 'Ø¸Ù†ÙÙ‘',\n",
       " 'Ø¹Ø¯ÙÙ‘',\n",
       " 'Ø¹Ù„Ù…',\n",
       " 'ØºØ§Ø¯Ø±',\n",
       " 'Ø°Ù‡Ø¨',\n",
       " 'ÙˆØ¬Ø¯',\n",
       " 'ÙˆØ±Ø¯',\n",
       " 'ÙˆÙ‡Ø¨',\n",
       " 'Ø£Ø³ÙƒÙ†',\n",
       " 'Ø£Ø·Ø¹Ù…',\n",
       " 'Ø£Ø¹Ø·Ù‰',\n",
       " 'Ø±Ø²Ù‚',\n",
       " 'Ø²ÙˆØ¯',\n",
       " 'Ø³Ù‚Ù‰',\n",
       " 'ÙƒØ³Ø§',\n",
       " 'Ø£Ø®Ø¨Ø±',\n",
       " 'Ø£Ø±Ù‰',\n",
       " 'Ø£Ø¹Ù„Ù…',\n",
       " 'Ø£Ù†Ø¨Ø£',\n",
       " 'Ø­Ø¯ÙØ«',\n",
       " 'Ø®Ø¨ÙÙ‘Ø±',\n",
       " 'Ù†Ø¨ÙÙ‘Ø§',\n",
       " 'Ø£ÙØ¹Ù„ Ø¨Ù‡',\n",
       " 'Ù…Ø§ Ø£ÙØ¹Ù„Ù‡',\n",
       " 'Ø¨Ø¦Ø³',\n",
       " 'Ø³Ø§Ø¡',\n",
       " 'Ø·Ø§Ù„Ù…Ø§',\n",
       " 'Ù‚Ù„Ù…Ø§',\n",
       " 'Ù„Ø§Øª',\n",
       " 'Ù„ÙƒÙ†ÙÙ‘',\n",
       " 'Ø¡Ù',\n",
       " 'Ø£Ø¬Ù„',\n",
       " 'Ø¥Ø°Ø§Ù‹',\n",
       " 'Ø£Ù…Ù‘Ø§',\n",
       " 'Ø¥Ù…Ù‘Ø§',\n",
       " 'Ø¥Ù†ÙÙ‘',\n",
       " 'Ø£Ù†Ù‹Ù‘',\n",
       " 'Ø£Ù‰',\n",
       " 'Ø¥Ù‰',\n",
       " 'Ø£ÙŠØ§',\n",
       " 'Ø¨',\n",
       " 'Ø«Ù…ÙÙ‘',\n",
       " 'Ø¬Ù„Ù„',\n",
       " 'Ø¬ÙŠØ±',\n",
       " 'Ø±ÙØ¨ÙÙ‘',\n",
       " 'Ø³',\n",
       " 'Ø¹Ù„Ù‹Ù‘',\n",
       " 'Ù',\n",
       " 'ÙƒØ£Ù†Ù‘',\n",
       " 'ÙƒÙ„ÙÙ‘Ø§',\n",
       " 'ÙƒÙ‰',\n",
       " 'Ù„',\n",
       " 'Ù„Ø§Øª',\n",
       " 'Ù„Ø¹Ù„ÙÙ‘',\n",
       " 'Ù„ÙƒÙ†ÙÙ‘',\n",
       " 'Ù„ÙƒÙ†ÙÙ‘',\n",
       " 'Ù…',\n",
       " 'Ù†ÙÙ‘',\n",
       " 'Ù‡Ù„Ù‘Ø§',\n",
       " 'ÙˆØ§',\n",
       " 'Ø£Ù„',\n",
       " 'Ø¥Ù„Ù‘Ø§',\n",
       " 'Øª',\n",
       " 'Ùƒ',\n",
       " 'Ù„Ù…Ù‘Ø§',\n",
       " 'Ù†',\n",
       " 'Ù‡',\n",
       " 'Ùˆ',\n",
       " 'Ø§',\n",
       " 'ÙŠ',\n",
       " 'ØªØ¬Ø§Ù‡',\n",
       " 'ØªÙ„Ù‚Ø§Ø¡',\n",
       " 'Ø¬Ù…ÙŠØ¹',\n",
       " 'Ø­Ø³Ø¨',\n",
       " 'Ø³Ø¨Ø­Ø§Ù†',\n",
       " 'Ø´Ø¨Ù‡',\n",
       " 'Ù„Ø¹Ù…Ø±',\n",
       " 'Ù…Ø«Ù„',\n",
       " 'Ù…Ø¹Ø§Ø°',\n",
       " 'Ø£Ø¨Ùˆ',\n",
       " 'Ø£Ø®Ùˆ',\n",
       " 'Ø­Ù…Ùˆ',\n",
       " 'ÙÙˆ',\n",
       " 'Ù…Ø¦Ø©',\n",
       " 'Ù…Ø¦ØªØ§Ù†',\n",
       " 'Ø«Ù„Ø§Ø«Ù…Ø¦Ø©',\n",
       " 'Ø£Ø±Ø¨Ø¹Ù…Ø¦Ø©',\n",
       " 'Ø®Ù…Ø³Ù…Ø¦Ø©',\n",
       " 'Ø³ØªÙ…Ø¦Ø©',\n",
       " 'Ø³Ø¨Ø¹Ù…Ø¦Ø©',\n",
       " 'Ø«Ù…Ù†Ù…Ø¦Ø©',\n",
       " 'ØªØ³Ø¹Ù…Ø¦Ø©',\n",
       " 'Ù…Ø§Ø¦Ø©',\n",
       " 'Ø«Ù„Ø§Ø«Ù…Ø§Ø¦Ø©',\n",
       " 'Ø£Ø±Ø¨Ø¹Ù…Ø§Ø¦Ø©',\n",
       " 'Ø®Ù…Ø³Ù…Ø§Ø¦Ø©',\n",
       " 'Ø³ØªÙ…Ø§Ø¦Ø©',\n",
       " 'Ø³Ø¨Ø¹Ù…Ø§Ø¦Ø©',\n",
       " 'Ø«Ù…Ø§Ù†Ù…Ø¦Ø©',\n",
       " 'ØªØ³Ø¹Ù…Ø§Ø¦Ø©',\n",
       " 'Ø¹Ø´Ø±ÙˆÙ†',\n",
       " 'Ø«Ù„Ø§Ø«ÙˆÙ†',\n",
       " 'Ø§Ø±Ø¨Ø¹ÙˆÙ†',\n",
       " 'Ø®Ù…Ø³ÙˆÙ†',\n",
       " 'Ø³ØªÙˆÙ†',\n",
       " 'Ø³Ø¨Ø¹ÙˆÙ†',\n",
       " 'Ø«Ù…Ø§Ù†ÙˆÙ†',\n",
       " 'ØªØ³Ø¹ÙˆÙ†',\n",
       " 'Ø¹Ø´Ø±ÙŠÙ†',\n",
       " 'Ø«Ù„Ø§Ø«ÙŠÙ†',\n",
       " 'Ø§Ø±Ø¨Ø¹ÙŠÙ†',\n",
       " 'Ø®Ù…Ø³ÙŠÙ†',\n",
       " 'Ø³ØªÙŠÙ†',\n",
       " 'Ø³Ø¨Ø¹ÙŠÙ†',\n",
       " 'Ø«Ù…Ø§Ù†ÙŠÙ†',\n",
       " 'ØªØ³Ø¹ÙŠÙ†',\n",
       " 'Ø¨Ø¶Ø¹',\n",
       " 'Ù†ÙŠÙ',\n",
       " 'Ø£Ø¬Ù…Ø¹',\n",
       " 'Ø¬Ù…ÙŠØ¹',\n",
       " 'Ø¹Ø§Ù…Ø©',\n",
       " 'Ø¹ÙŠÙ†',\n",
       " 'Ù†ÙØ³',\n",
       " 'Ù„Ø§ Ø³ÙŠÙ…Ø§',\n",
       " 'Ø£ØµÙ„Ø§',\n",
       " 'Ø£Ù‡Ù„Ø§',\n",
       " 'Ø£ÙŠØ¶Ø§',\n",
       " 'Ø¨Ø¤Ø³Ø§',\n",
       " 'Ø¨Ø¹Ø¯Ø§',\n",
       " 'Ø¨ØºØªØ©',\n",
       " 'ØªØ¹Ø³Ø§',\n",
       " 'Ø­Ù‚Ø§',\n",
       " 'Ø­Ù…Ø¯Ø§',\n",
       " 'Ø®Ù„Ø§ÙØ§',\n",
       " 'Ø®Ø§ØµØ©',\n",
       " 'Ø¯ÙˆØ§Ù„ÙŠÙƒ',\n",
       " 'Ø³Ø­Ù‚Ø§',\n",
       " 'Ø³Ø±Ø§',\n",
       " 'Ø³Ù…Ø¹Ø§',\n",
       " 'ØµØ¨Ø±Ø§',\n",
       " 'ØµØ¯Ù‚Ø§',\n",
       " 'ØµØ±Ø§Ø­Ø©',\n",
       " 'Ø·Ø±Ø§',\n",
       " 'Ø¹Ø¬Ø¨Ø§',\n",
       " 'Ø¹ÙŠØ§Ù†Ø§',\n",
       " 'ØºØ§Ù„Ø¨Ø§',\n",
       " 'ÙØ±Ø§Ø¯Ù‰',\n",
       " 'ÙØ¶Ù„Ø§',\n",
       " 'Ù‚Ø§Ø·Ø¨Ø©',\n",
       " 'ÙƒØ«ÙŠØ±Ø§',\n",
       " 'Ù„Ø¨ÙŠÙƒ',\n",
       " 'Ù…Ø¹Ø§Ø°',\n",
       " 'Ø£Ø¨Ø¯Ø§',\n",
       " 'Ø¥Ø²Ø§Ø¡',\n",
       " 'Ø£ØµÙ„Ø§',\n",
       " 'Ø§Ù„Ø¢Ù†',\n",
       " 'Ø£Ù…Ø¯',\n",
       " 'Ø£Ù…Ø³',\n",
       " 'Ø¢Ù†ÙØ§',\n",
       " 'Ø¢Ù†Ø§Ø¡',\n",
       " 'Ø£Ù†Ù‘Ù‰',\n",
       " 'Ø£ÙˆÙ„',\n",
       " 'Ø£ÙŠÙ‘Ø§Ù†',\n",
       " 'ØªØ§Ø±Ø©',\n",
       " 'Ø«Ù…Ù‘',\n",
       " 'Ø«Ù…Ù‘Ø©',\n",
       " 'Ø­Ù‚Ø§',\n",
       " 'ØµØ¨Ø§Ø­',\n",
       " 'Ù…Ø³Ø§Ø¡',\n",
       " 'Ø¶Ø­ÙˆØ©',\n",
       " 'Ø¹ÙˆØ¶',\n",
       " 'ØºØ¯Ø§',\n",
       " 'ØºØ¯Ø§Ø©',\n",
       " 'Ù‚Ø·Ù‘',\n",
       " 'ÙƒÙ„Ù‘Ù…Ø§',\n",
       " 'Ù„Ø¯Ù†',\n",
       " 'Ù„Ù…Ù‘Ø§',\n",
       " 'Ù…Ø±Ù‘Ø©',\n",
       " 'Ù‚Ø¨Ù„',\n",
       " 'Ø®Ù„Ù',\n",
       " 'Ø£Ù…Ø§Ù…',\n",
       " 'ÙÙˆÙ‚',\n",
       " 'ØªØ­Øª',\n",
       " 'ÙŠÙ…ÙŠÙ†',\n",
       " 'Ø´Ù…Ø§Ù„',\n",
       " 'Ø§Ø±ØªØ¯Ù‘',\n",
       " 'Ø§Ø³ØªØ­Ø§Ù„',\n",
       " 'Ø£ØµØ¨Ø­',\n",
       " 'Ø£Ø¶Ø­Ù‰',\n",
       " 'Ø¢Ø¶',\n",
       " 'Ø£Ù…Ø³Ù‰',\n",
       " 'Ø§Ù†Ù‚Ù„Ø¨',\n",
       " 'Ø¨Ø§Øª',\n",
       " 'ØªØ¨Ø¯Ù‘Ù„',\n",
       " 'ØªØ­ÙˆÙ‘Ù„',\n",
       " 'Ø­Ø§Ø±',\n",
       " 'Ø±Ø¬Ø¹',\n",
       " 'Ø±Ø§Ø­',\n",
       " 'ØµØ§Ø±',\n",
       " 'Ø¸Ù„Ù‘',\n",
       " 'Ø¹Ø§Ø¯',\n",
       " 'ØºØ¯Ø§',\n",
       " 'ÙƒØ§Ù†',\n",
       " 'Ù…Ø§ Ø§Ù†ÙÙƒ',\n",
       " 'Ù…Ø§ Ø¨Ø±Ø­',\n",
       " 'Ù…Ø§Ø¯Ø§Ù…',\n",
       " 'Ù…Ø§Ø²Ø§Ù„',\n",
       " 'Ù…Ø§ÙØªØ¦',\n",
       " 'Ø§Ø¨ØªØ¯Ø£',\n",
       " 'Ø£Ø®Ø°',\n",
       " 'Ø§Ø®Ù„ÙˆÙ„Ù‚',\n",
       " 'Ø£Ù‚Ø¨Ù„',\n",
       " 'Ø§Ù†Ø¨Ø±Ù‰',\n",
       " 'Ø£Ù†Ø´Ø£',\n",
       " 'Ø£ÙˆØ´Ùƒ',\n",
       " 'Ø¬Ø¹Ù„',\n",
       " 'Ø­Ø±Ù‰',\n",
       " 'Ø´Ø±Ø¹',\n",
       " 'Ø·ÙÙ‚',\n",
       " 'Ø¹Ù„Ù‚',\n",
       " 'Ù‚Ø§Ù…',\n",
       " 'ÙƒØ±Ø¨',\n",
       " 'ÙƒØ§Ø¯',\n",
       " 'Ù‡Ø¨Ù‘']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for Indiaâ€™s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isnâ€™t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_sentences = nltk.sent_tokenize(paragraph)\n",
    "speech_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'three',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'India',\n",
       " '.',\n",
       " 'In',\n",
       " '3000',\n",
       " 'years',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history',\n",
       " ',',\n",
       " 'people',\n",
       " 'from',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " 'have',\n",
       " 'come',\n",
       " 'and',\n",
       " 'invaded',\n",
       " 'us',\n",
       " ',',\n",
       " 'captured',\n",
       " 'our',\n",
       " 'lands',\n",
       " ',',\n",
       " 'conquered',\n",
       " 'our',\n",
       " 'minds',\n",
       " '.',\n",
       " 'From',\n",
       " 'Alexander',\n",
       " 'onwards',\n",
       " ',',\n",
       " 'the',\n",
       " 'Greeks',\n",
       " ',',\n",
       " 'the',\n",
       " 'Turks',\n",
       " ',',\n",
       " 'the',\n",
       " 'Moguls',\n",
       " ',',\n",
       " 'the',\n",
       " 'Portuguese',\n",
       " ',',\n",
       " 'the',\n",
       " 'British',\n",
       " ',',\n",
       " 'the',\n",
       " 'French',\n",
       " ',',\n",
       " 'the',\n",
       " 'Dutch',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'them',\n",
       " 'came',\n",
       " 'and',\n",
       " 'looted',\n",
       " 'us',\n",
       " ',',\n",
       " 'took',\n",
       " 'over',\n",
       " 'what',\n",
       " 'was',\n",
       " 'ours',\n",
       " '.',\n",
       " 'Yet',\n",
       " 'we',\n",
       " 'have',\n",
       " 'not',\n",
       " 'done',\n",
       " 'this',\n",
       " 'to',\n",
       " 'any',\n",
       " 'other',\n",
       " 'nation',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'not',\n",
       " 'conquered',\n",
       " 'anyone',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'not',\n",
       " 'grabbed',\n",
       " 'their',\n",
       " 'land',\n",
       " ',',\n",
       " 'their',\n",
       " 'culture',\n",
       " ',',\n",
       " 'their',\n",
       " 'history',\n",
       " 'and',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'enforce',\n",
       " 'our',\n",
       " 'way',\n",
       " 'of',\n",
       " 'life',\n",
       " 'on',\n",
       " 'them',\n",
       " '.',\n",
       " 'Why',\n",
       " '?',\n",
       " 'Because',\n",
       " 'we',\n",
       " 'respect',\n",
       " 'the',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'others.That',\n",
       " 'is',\n",
       " 'why',\n",
       " 'my',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'is',\n",
       " 'that',\n",
       " 'of',\n",
       " 'freedom',\n",
       " '.',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'India',\n",
       " 'got',\n",
       " 'its',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'of',\n",
       " 'this',\n",
       " 'in',\n",
       " '1857',\n",
       " ',',\n",
       " 'when',\n",
       " 'we',\n",
       " 'started',\n",
       " 'the',\n",
       " 'War',\n",
       " 'of',\n",
       " 'Independence',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'this',\n",
       " 'freedom',\n",
       " 'that',\n",
       " 'we',\n",
       " 'must',\n",
       " 'protect',\n",
       " 'and',\n",
       " 'nurture',\n",
       " 'and',\n",
       " 'build',\n",
       " 'on',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'not',\n",
       " 'free',\n",
       " ',',\n",
       " 'no',\n",
       " 'one',\n",
       " 'will',\n",
       " 'respect',\n",
       " 'us',\n",
       " '.',\n",
       " 'My',\n",
       " 'second',\n",
       " 'vision',\n",
       " 'for',\n",
       " 'India',\n",
       " 'â€™',\n",
       " 's',\n",
       " 'development',\n",
       " '.',\n",
       " 'For',\n",
       " 'fifty',\n",
       " 'years',\n",
       " 'we',\n",
       " 'have',\n",
       " 'been',\n",
       " 'a',\n",
       " 'developing',\n",
       " 'nation',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'time',\n",
       " 'we',\n",
       " 'see',\n",
       " 'ourselves',\n",
       " 'as',\n",
       " 'a',\n",
       " 'developed',\n",
       " 'nation',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'among',\n",
       " 'the',\n",
       " 'top',\n",
       " '5',\n",
       " 'nations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'GDP',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'a',\n",
       " '10',\n",
       " 'percent',\n",
       " 'growth',\n",
       " 'rate',\n",
       " 'in',\n",
       " 'most',\n",
       " 'areas',\n",
       " '.',\n",
       " 'Our',\n",
       " 'poverty',\n",
       " 'levels',\n",
       " 'are',\n",
       " 'falling',\n",
       " '.',\n",
       " 'Our',\n",
       " 'achievements',\n",
       " 'are',\n",
       " 'being',\n",
       " 'globally',\n",
       " 'recognised',\n",
       " 'today',\n",
       " '.',\n",
       " 'Yet',\n",
       " 'we',\n",
       " 'lack',\n",
       " 'the',\n",
       " 'self-confidence',\n",
       " 'to',\n",
       " 'see',\n",
       " 'ourselves',\n",
       " 'as',\n",
       " 'a',\n",
       " 'developed',\n",
       " 'nation',\n",
       " ',',\n",
       " 'self-reliant',\n",
       " 'and',\n",
       " 'self-assured',\n",
       " '.',\n",
       " 'Isn',\n",
       " 'â€™',\n",
       " 't',\n",
       " 'this',\n",
       " 'incorrect',\n",
       " '?',\n",
       " 'I',\n",
       " 'have',\n",
       " 'a',\n",
       " 'third',\n",
       " 'vision',\n",
       " '.',\n",
       " 'India',\n",
       " 'must',\n",
       " 'stand',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Because',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'unless',\n",
       " 'India',\n",
       " 'stands',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'no',\n",
       " 'one',\n",
       " 'will',\n",
       " 'respect',\n",
       " 'us',\n",
       " '.',\n",
       " 'Only',\n",
       " 'strength',\n",
       " 'respects',\n",
       " 'strength',\n",
       " '.',\n",
       " 'We',\n",
       " 'must',\n",
       " 'be',\n",
       " 'strong',\n",
       " 'not',\n",
       " 'only',\n",
       " 'as',\n",
       " 'a',\n",
       " 'military',\n",
       " 'power',\n",
       " 'but',\n",
       " 'also',\n",
       " 'as',\n",
       " 'an',\n",
       " 'economic',\n",
       " 'power',\n",
       " '.',\n",
       " 'Both',\n",
       " 'must',\n",
       " 'go',\n",
       " 'hand-in-hand',\n",
       " '.',\n",
       " 'My',\n",
       " 'good',\n",
       " 'fortune',\n",
       " 'was',\n",
       " 'to',\n",
       " 'have',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'three',\n",
       " 'great',\n",
       " 'minds',\n",
       " '.',\n",
       " 'Dr.',\n",
       " 'Vikram',\n",
       " 'Sarabhai',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Dept',\n",
       " '.',\n",
       " 'of',\n",
       " 'space',\n",
       " ',',\n",
       " 'Professor',\n",
       " 'Satish',\n",
       " 'Dhawan',\n",
       " ',',\n",
       " 'who',\n",
       " 'succeeded',\n",
       " 'him',\n",
       " 'and',\n",
       " 'Dr.',\n",
       " 'Brahm',\n",
       " 'Prakash',\n",
       " ',',\n",
       " 'father',\n",
       " 'of',\n",
       " 'nuclear',\n",
       " 'material',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'lucky',\n",
       " 'to',\n",
       " 'have',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'all',\n",
       " 'three',\n",
       " 'of',\n",
       " 'them',\n",
       " 'closely',\n",
       " 'and',\n",
       " 'consider',\n",
       " 'this',\n",
       " 'the',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life',\n",
       " '.',\n",
       " 'I',\n",
       " 'see',\n",
       " 'four',\n",
       " 'milestones',\n",
       " 'in',\n",
       " 'my',\n",
       " 'career']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_words = nltk.word_tokenize(paragraph)\n",
    "speech_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Stopwords and filter and then apply stemming or lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india â€™ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn â€™ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(speech_sentences)):\n",
    "    words = nltk.word_tokenize(speech_sentences[i])\n",
    "    # words = [word for word in words if word not in eng_stopwords]\n",
    "    words = [porter_stemmer.stem(word) for word in words if word not in eng_stopwords]\n",
    "    speech_sentences[i] = ' '.join(words) # join the words to form a sentence\n",
    "speech_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Using Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india â€™ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn â€™ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(speech_sentences)):\n",
    "    words = nltk.word_tokenize(speech_sentences[i])\n",
    "    # words = [word for word in words if word not in eng_stopwords]\n",
    "    words = [snowball_stemmer.stem(word) for word in words if word not in eng_stopwords]\n",
    "    speech_sentences[i] = ' '.join(words) # join the words to form a sentence\n",
    "speech_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Using Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'from alexander onwards , greek , turk , mogul , portuguese , british , french , dutch , came looted u , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquered anyone .',\n",
       " 'we grabbed land , culture , history tried enforce way life .',\n",
       " 'why ?',\n",
       " 'because respect freedom others.that first vision freedom .',\n",
       " 'i believe india got first vision 1857 , started war independence .',\n",
       " 'it freedom must protect nurture build .',\n",
       " 'if free , one respect u .',\n",
       " 'my second vision india â€™ development .',\n",
       " 'for fifty year developing nation .',\n",
       " 'it time see developed nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverty level falling .',\n",
       " 'our achievement globally recognised today .',\n",
       " 'yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
       " 'isn â€™ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'because i believe unless india stand world , one respect u .',\n",
       " 'only strength respect strength .',\n",
       " 'we must strong military power also economic power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortune worked three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeeded dr. brahm prakash , father nuclear material .',\n",
       " 'i lucky worked three closely consider great opportunity life .',\n",
       " 'i see four milestone career']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(speech_sentences)):\n",
    "    words = nltk.word_tokenize(speech_sentences[i])\n",
    "    # words = [word for word in words if word not in eng_stopwords]\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in eng_stopwords]\n",
    "    speech_sentences[i] = ' '.join(words) # join the words to form a sentence\n",
    "speech_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Why Remove Stopwords?**\n",
    "\n",
    "âœ… **Reduces dataset size** â†’ Speeds up processing.  \n",
    "âœ… **Improves accuracy** â†’ Less noise in text analysis.  \n",
    "âœ… **Enhances model performance** â†’ Focuses on important words.  \n",
    "\n",
    "---\n",
    "\n",
    "**When NOT to Remove Stopwords?**\n",
    "\n",
    "âŒ If stopwords **carry meaning** (e.g., in **sentiment analysis** or **question classification**).  \n",
    "âŒ If you are working with **short texts** (e.g., chatbot responses).  \n",
    "\n",
    "---\n",
    "\n",
    "**Customizing Stopwords List**\n",
    "\n",
    "You can **add or remove** stopwords based on your use case.\n",
    "\n",
    "```python\n",
    "# Add a custom stopword\n",
    "stop_words.add(\"example\")\n",
    "\n",
    "# Remove a stopword\n",
    "stop_words.remove(\"not\")  # Useful if you need negation in sentiment analysis\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samarth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
