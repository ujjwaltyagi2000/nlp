{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world!',\n",
       " 'This is a sample text for natural language processing.',\n",
       " \"We are going to perform tokenization on it using NLTK's tokenizer functionality.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"Hello world! This is a sample text for natural language processing. \n",
    "We are going to perform tokenization on it using NLTK's tokenizer functionality.\"\"\"\n",
    "# tokenizes by sentence.\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences) # number of sentences in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.', 'We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'s\", 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) # number of words and punctuations in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!']\n",
      "['This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.']\n",
      "['We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'s\", 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "# print words of each sentence\n",
    "for sentence in sentences:\n",
    "    print(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'natural', 'language', 'processing', '.', 'We', 'are', 'going', 'to', 'perform', 'tokenization', 'on', 'it', 'using', 'NLTK', \"'\", 's', 'tokenizer', 'functionality', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between the above word_tokenize and wordpunct_tokenize is that in the previous example the apostrophe _**\" ' \"**_ did non get split separately. But in this example, it got separated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'world',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'tokenization',\n",
       " 'on',\n",
       " 'it',\n",
       " 'using',\n",
       " 'NLTK',\n",
       " \"'s\",\n",
       " 'tokenizer',\n",
       " 'functionality',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides several word tokenizers, each suited for different use cases. Here are the key tokenizers and their differences:\n",
    "\n",
    "##### 1. **`word_tokenize` (Recommended)**\n",
    "   - **Implementation:** Uses the Penn Treebank tokenizer from `nltk.tokenize.punkt`.\n",
    "   - **Features:** Handles punctuation, contractions, and special cases like \"U.S.\" correctly.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import word_tokenize\n",
    "     text = \"I'm going to the U.S. next week!\"\n",
    "     print(word_tokenize(text))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     [\"I\", \"'m\", \"going\", \"to\", \"the\", \"U.S.\", \"next\", \"week\", \"!\"]\n",
    "     ```\n",
    "   - **Use Case:** General-purpose word tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **`TreebankWordTokenizer`**\n",
    "   - **Implementation:** Uses the Penn Treebank tokenizer rules (same as `word_tokenize`).\n",
    "   - **Features:** Splits contractions (e.g., \"can't\" → [\"ca\", \"n't\"]), handles punctuation.\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import TreebankWordTokenizer\n",
    "     tokenizer = TreebankWordTokenizer()\n",
    "     print(tokenizer.tokenize(\"Can't won't don't\"))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     ['Ca', \"n't\", 'wo', \"n't\", 'do', \"n't\"]\n",
    "     ```\n",
    "   - **Use Case:** When working with text where contractions need to be split.\n",
    "\n",
    "A **contraction** is a shortened form of one or more words where missing letters are replaced by an apostrophe (`'`). Contractions are commonly used in informal writing and speech.  \n",
    "\n",
    "##### **Examples of Contractions:**\n",
    "| Full Form | Contraction |\n",
    "|-----------|------------|\n",
    "| I am | I'm |\n",
    "| You are | You're |\n",
    "| He is / He has | He's |\n",
    "| They are | They're |\n",
    "| Cannot | Can't |\n",
    "| Will not | Won't |\n",
    "| Do not | Don't |\n",
    "| Should not | Shouldn't |\n",
    "| Would have | Would've |\n",
    "\n",
    "##### **Why Do Contractions Matter in NLP?**\n",
    "- Some tokenizers **split contractions** into separate words (`\"can't\"` → `[\"ca\", \"n't\"]`).\n",
    "- Others **keep contractions intact** (`\"can't\"` → `[\"can't\"]`).\n",
    "- Handling contractions correctly is important for **sentiment analysis**, **text preprocessing**, and **machine learning models**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### 3. **`WordPunctTokenizer`**\n",
    "   - **Implementation:** Splits words and punctuation separately.\n",
    "   - **Features:** Breaks contractions into separate parts (e.g., \"can't\" → [\"can\", \"'t\"]).\n",
    "   - **Example:**\n",
    "     ```python\n",
    "     from nltk.tokenize import WordPunctTokenizer\n",
    "     tokenizer = WordPunctTokenizer()\n",
    "     print(tokenizer.tokenize(\"I'm excited!\"))\n",
    "     ```\n",
    "     **Output:**\n",
    "     ```python\n",
    "     ['I', \"'\", 'm', 'excited', '!']\n",
    "     ```\n",
    "   - **Use Case:** When punctuation needs to be treated as separate tokens.\n",
    "\n",
    "---\n",
    "There are a couple other tokenizers.\n",
    "\n",
    "##### **Summary**\n",
    "| Tokenizer | Handles Contractions? | Splits Punctuation? | Use Case |\n",
    "|-----------|------------------|----------------|---------|\n",
    "| `word_tokenize` | Yes | Mostly | General NLP tasks |\n",
    "| `TreebankWordTokenizer` | Yes (splits aggressively) | Yes | Similar to `word_tokenize`, more aggressive |\n",
    "| `ToktokTokenizer` | No | Yes | Fast and simple tokenization |\n",
    "| `RegexpTokenizer` | Custom | Custom | Custom rules-based tokenization |\n",
    "| `WordPunctTokenizer` | Yes (aggressive) | Yes (separates all punctuation) | When punctuation should be separate |\n",
    "| `MWETokenizer` | No | No | Preserve multi-word expressions |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** is the process of reducing a word to its root form (also called a \"stem\") by **removing suffixes**. It helps normalize words so that variations of a word are treated as the same.  \n",
    "\n",
    "For example:  \n",
    "- **\"running\" → \"run\"**  \n",
    "- **\"flies\" → \"fli\"** (incorrect but common with some stemmers)  \n",
    "- **\"happily\" → \"happili\"**  \n",
    "\n",
    "Stemming is a **rule-based** approach and doesn't always produce real words. It just chops off endings based on predefined rules.\n",
    "\n",
    "---\n",
    "\n",
    "**Common Stemming Algorithms in NLTK**\n",
    "1️⃣ **Porter Stemmer** (Most Common)  \n",
    "Uses a set of heuristic rules to remove suffixes. It is simple but sometimes over-stems words.  \n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"running\"))   # Output: run\n",
    "print(stemmer.stem(\"flies\"))     # Output: fli\n",
    "print(stemmer.stem(\"happiness\")) # Output: happi\n",
    "```\n",
    "✅ **Pros:** Fast, widely used  \n",
    "❌ **Cons:** Sometimes removes too much, resulting in non-real words  \n",
    "\n",
    "---\n",
    "2️⃣ **Lancaster Stemmer** (Aggressive)  \n",
    "More aggressive than Porter Stemmer, often reducing words excessively.  \n",
    "```python\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem(\"running\"))   # Output: run\n",
    "print(stemmer.stem(\"flies\"))     # Output: fli\n",
    "print(stemmer.stem(\"happiness\")) # Output: happy\n",
    "```\n",
    "✅ **Pros:** More aggressive  \n",
    "❌ **Cons:** Can distort words too much  \n",
    "\n",
    "---\n",
    "\n",
    "3️⃣ **Snowball Stemmer** (Improved Porter Stemmer)  \n",
    "A more advanced version of the Porter Stemmer, supporting multiple languages.  \n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))   # Output: run\n",
    "print(stemmer.stem(\"flies\"))     # Output: fli\n",
    "print(stemmer.stem(\"happiness\")) # Output: happi\n",
    "```\n",
    "✅ **Pros:** Better than Porter, supports many languages  \n",
    "❌ **Cons:** Still rule-based, may not handle all cases correctly  \n",
    "\n",
    "---\n",
    "\n",
    "**Limitations of Stemming**\n",
    "- Doesn't always produce real words (e.g., *\"flies\"* → *\"fli\"*)  \n",
    "- Different words may map to the same stem incorrectly  \n",
    "- Over-stemming (too aggressive) or under-stemming (not aggressive enough)  \n",
    "\n",
    "---\n",
    "\n",
    " **Stemming vs Lemmatization**\n",
    "If you need more **accurate** root words, **lemmatization** is a better choice because it uses a **dictionary-based** approach instead of just chopping off suffixes.\n",
    "\n",
    "Would you like to explore **lemmatization** next? 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samarth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
