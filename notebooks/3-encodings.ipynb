{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Encodings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in text preprocessing is to vectorize the filtered tokens so they can be used to build models. The vectorization requires words in a corpus to be given numeric values. There are multiple techniques used to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a way to represent words (or categorical data) as **binary vectors**. Each word is assigned a **unique index**, and its vector has **1 at its position** while all other positions are **0**.\n",
    "\n",
    "For example, if we have the words:  \n",
    "**[\"cat\", \"dog\", \"fish\"]**, we can represent them as:\n",
    "\n",
    "| Word     | cat | dog | fish |\n",
    "| -------- | --- | --- | ---- |\n",
    "| **cat**  | 1   | 0   | 0    |\n",
    "| **dog**  | 0   | 1   | 0    |\n",
    "| **fish** | 0   | 0   | 1    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of One Hot Encoding is really simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is awesome\",\n",
    "    \"I love Machine Learning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: [0. 0. 1. 0. 0. 0. 0.]\n",
      "love: [0. 0. 0. 0. 0. 0. 1.]\n",
      "I: [1. 0. 0. 0. 0. 0. 0.]\n",
      "is: [0. 0. 0. 0. 0. 1. 0.]\n",
      "Learning: [0. 1. 0. 0. 0. 0. 0.]\n",
      "NLP: [0. 0. 0. 1. 0. 0. 0.]\n",
      "awesome: [0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\"I love NLP\", \"NLP is awesome\", \"I love Machine Learning\"]\n",
    "\n",
    "# Convert text into a list of unique words\n",
    "unique_words = list(set(\" \".join(corpus).split()))\n",
    "\n",
    "# Convert words into a one-hot encoded format\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)  # âœ… Fixed!\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(np.array(unique_words).reshape(-1, 1))\n",
    "\n",
    "# Display results\n",
    "for word, encoding in zip(unique_words, one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) is a simple way to **convert text into numerical features** for machine learning models. It represents **how frequently words appear** in a document **without considering their order** or context.\n",
    "\n",
    "**ðŸ“Œ How BoW Works**\n",
    "\n",
    "1ï¸âƒ£ **Create a Vocabulary** (a set of unique words in the dataset).  \n",
    "2ï¸âƒ£ **Count Word Occurrences** in each document.  \n",
    "3ï¸âƒ£ **Convert into a Vector Representation.**\n",
    "\n",
    "**Example**:  \n",
    "Consider these two sentences:  \n",
    "ðŸ“Œ **Sentence 1:** \"I love NLP and Machine Learning.\"  \n",
    "ðŸ“Œ **Sentence 2:** \"I love Deep Learning and NLP.\"\n",
    "\n",
    "ðŸ”¹ **Step 1: Create a Vocabulary**  \n",
    "Unique words in both sentences â†’  \n",
    "ðŸ“Œ **[\"I\", \"love\", \"NLP\", \"and\", \"Machine\", \"Learning\", \"Deep\"]**\n",
    "\n",
    "ðŸ”¹ **Step 2: Create Word Frequency Vectors**\n",
    "\n",
    "|                | I   | love | NLP | and | Machine | Learning | Deep |\n",
    "| -------------- | --- | ---- | --- | --- | ------- | -------- | ---- |\n",
    "| **Sentence 1** | 1   | 1    | 1   | 1   | 1       | 1        | 0    |\n",
    "| **Sentence 2** | 1   | 1    | 1   | 1   | 0       | 1        | 1    |\n",
    "\n",
    "Each row is a **numerical representation of a sentence** based on word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome' 'is' 'learning' 'love' 'machine' 'nlp']\n",
      "[[0 0 0 1 0 1]\n",
      " [1 1 0 0 0 1]\n",
      " [0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to array for readability\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow_encoded.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    "[[0 0 0 1 0 1]   -> Sentence 1: Contains 'love' and 'nlp'\n",
    "\n",
    " [1 1 0 0 0 1]   -> Sentence 2: Contains 'awesome', 'is', and 'nlp'\n",
    "\n",
    " [0 0 1 1 1 0]]  -> Sentence 3: Contains 'learning', 'love', and 'machine'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samarth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
