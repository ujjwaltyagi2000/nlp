{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Encodings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in text preprocessing is to vectorize the filtered tokens so they can be used to build models. The vectorization requires words in a corpus to be given numeric values. There are multiple techniques used to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a way to represent words (or categorical data) as **binary vectors**. Each word is assigned a **unique index**, and its vector has **1 at its position** while all other positions are **0**.\n",
    "\n",
    "For example, if we have the words:  \n",
    "**[\"cat\", \"dog\", \"fish\"]**, we can represent them as:\n",
    "\n",
    "| Word     | cat | dog | fish |\n",
    "| -------- | --- | --- | ---- |\n",
    "| **cat**  | 1   | 0   | 0    |\n",
    "| **dog**  | 0   | 1   | 0    |\n",
    "| **fish** | 0   | 0   | 1    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of One Hot Encoding is really simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is awesome\",\n",
    "    \"I love Machine Learning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: [0. 0. 1. 0. 0. 0. 0.]\n",
      "love: [0. 0. 0. 0. 0. 0. 1.]\n",
      "I: [1. 0. 0. 0. 0. 0. 0.]\n",
      "is: [0. 0. 0. 0. 0. 1. 0.]\n",
      "Learning: [0. 1. 0. 0. 0. 0. 0.]\n",
      "NLP: [0. 0. 0. 1. 0. 0. 0.]\n",
      "awesome: [0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\"I love NLP\", \"NLP is awesome\", \"I love Machine Learning\"]\n",
    "\n",
    "# Convert text into a list of unique words\n",
    "unique_words = list(set(\" \".join(corpus).split()))\n",
    "\n",
    "# Convert words into a one-hot encoded format\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)  # âœ… Fixed!\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(np.array(unique_words).reshape(-1, 1))\n",
    "\n",
    "# Display results\n",
    "for word, encoding in zip(unique_words, one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) is a simple way to **convert text into numerical features** for machine learning models. It represents **how frequently words appear** in a document **without considering their order** or context.\n",
    "\n",
    "**ðŸ“Œ How BoW Works**\n",
    "\n",
    "1ï¸âƒ£ **Create a Vocabulary** (a set of unique words in the dataset).  \n",
    "2ï¸âƒ£ **Count Word Occurrences** in each document.  \n",
    "3ï¸âƒ£ **Convert into a Vector Representation.**\n",
    "\n",
    "**Example**:  \n",
    "Consider these two sentences:  \n",
    "ðŸ“Œ **Sentence 1:** \"I love NLP and Machine Learning.\"  \n",
    "ðŸ“Œ **Sentence 2:** \"I love Deep Learning and NLP.\"\n",
    "\n",
    "ðŸ”¹ **Step 1: Create a Vocabulary**  \n",
    "Unique words in both sentences â†’  \n",
    "ðŸ“Œ **[\"I\", \"love\", \"NLP\", \"and\", \"Machine\", \"Learning\", \"Deep\"]**\n",
    "\n",
    "ðŸ”¹ **Step 2: Create Word Frequency Vectors**\n",
    "\n",
    "|                | I   | love | NLP | and | Machine | Learning | Deep |\n",
    "| -------------- | --- | ---- | --- | --- | ------- | -------- | ---- |\n",
    "| **Sentence 1** | 1   | 1    | 1   | 1   | 1       | 1        | 0    |\n",
    "| **Sentence 2** | 1   | 1    | 1   | 1   | 0       | 1        | 1    |\n",
    "\n",
    "Each row is a **numerical representation of a sentence** based on word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome' 'is' 'learning' 'love' 'machine' 'nlp']\n",
      "[[0 0 0 1 0 1]\n",
      " [1 1 0 0 0 1]\n",
      " [0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to array for readability\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow_encoded.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    "[[0 0 0 1 0 1]   -> Sentence 1: Contains 'love' and 'nlp'\n",
    "\n",
    " [1 1 0 0 0 1]   -> Sentence 2: Contains 'awesome', 'is', and 'nlp'\n",
    "\n",
    " [0 0 1 1 1 0]]  -> Sentence 3: Contains 'learning', 'love', and 'machine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is widely used in **text mining, search engines, and NLP tasks** to find the most relevant words in a document.\n",
    "\n",
    "### **Formula Breakdown**\n",
    "\n",
    "TF-IDF is the product of two components:\n",
    "\n",
    "**1. Term Frequency (TF)**\n",
    "\n",
    "Measures how often a term appears in a document.\n",
    "\n",
    "$$\n",
    "TF = \\frac{\\text{Number of times a term appears in a document}}{\\text{Total number of terms in the document}}\n",
    "$$\n",
    "\n",
    "- **Example**: If the word \"machine\" appears **3 times** in a document with **100 words**, then:\n",
    "  $$\n",
    "  TF = \\frac{3}{100} = 0.03\n",
    "  $$\n",
    "\n",
    "**2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "Measures how important a word is by reducing the weight of commonly used words (e.g., \"the\", \"is\").\n",
    "\n",
    "$$\n",
    "IDF = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing the term}} + 1 \\right)\n",
    "$$\n",
    "\n",
    "- **Example**: If we have **10,000 documents** and the word \"machine\" appears in **1,000** of them:\n",
    "  $$\n",
    "  IDF = \\log \\left( \\frac{10,000}{1,000} + 1 \\right) = \\log(11) \\approx 2.4\n",
    "  $$\n",
    "\n",
    "**3. TF-IDF Calculation**\n",
    "\n",
    "$$\n",
    "TF-IDF = TF \\times IDF\n",
    "$$\n",
    "\n",
    "- If **TF = 0.03** and **IDF = 2.4**, then:\n",
    "  $$\n",
    "  TF-IDF = 0.03 \\times 2.4 = 0.072\n",
    "  $$\n",
    "- Higher values indicate that the word is **important** in the document but **rare** across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['about' 'ai' 'amazing' 'and' 'is' 'it' 'key' 'learning' 'love' 'machine'\n",
      " 'nlp' 'of' 'part']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.         0.46609584 0.         0.\n",
      "  0.         0.361965   0.46609584 0.46609584 0.46609584 0.\n",
      "  0.        ]\n",
      " [0.42024133 0.         0.42024133 0.31960436 0.31960436 0.42024133\n",
      "  0.         0.2482013  0.31960436 0.         0.31960436 0.\n",
      "  0.        ]\n",
      " [0.         0.4261835  0.         0.         0.32412354 0.\n",
      "  0.4261835  0.25171084 0.         0.32412354 0.         0.4261835\n",
      "  0.4261835 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love NLP and machine learning.\",\n",
    "    \"NLP is amazing and I love learning about it.\",\n",
    "    \"Machine learning is a key part of AI.\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to array and display results\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())  # Unique words\n",
    "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix.toarray())  # TF-IDF scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three rows in the matrix corresponsing to each sentence in the corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samarth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
