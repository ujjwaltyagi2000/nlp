{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional methods (frequency based embeddings) of representing words in a way that machines can understand, such as one-hot encoding, represent each word as a sparse vector with a dimension equal to the size of the vocabulary. Here, only one element of the vector is \"hot\" (set to 1) to indicate the presence of that word. While simple, this approach suffers from the curse of dimensionality, lacks semantic information and doesn't capture relationships between words.\n",
    "\n",
    "Prediction-based embeddings, on the other hand, are dense vectors with continuous values that are trained using machine learning techniques, often based on neural networks. The idea is to learn representations that encode semantic meaning and relationships between words. Word embeddings are trained by exposing a model to a large amount of text data and adjusting the vector representations based on the context in which words appear.\n",
    "\n",
    "One popular method for training prediction-based embeddings is Word2Vec, which uses a neural network to predict the surrounding words of a target word in a given context. Another widely used approach is GloVe (Global Vectors for Word Representation), which leverages global statistics to create embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Developed by a team of researchers at Google, including Tomas Mikolov, in 2013, Word2Vec (Word to Vector) has become a foundational technique for learning word embeddings in natural language processing (NLP) and machine learning models.\n",
    "\n",
    "Word2Vec consists of two main models for generating vector representations: Continuous Bag of Words (CBOW) and Continuous Skip-gram.\n",
    "\n",
    "In the context of Word2Vec, the Continuous Bag of Words (CBOW) model aims to predict a target word based on its surrounding context words within a given window. It uses the context words to predict the target word, and the learned embeddings capture semantic relationships between words.\n",
    "\n",
    "The Continuous Skip-gram model, on the other hand, takes a target word as input and aims to predict the surrounding context words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go step by step to understand **what exactly we did in the Word2Vec code**.\n",
    "\n",
    "### **1Ô∏è‚É£ Preprocessing the Data**  \n",
    "\n",
    "#### **Step 1: Import Libraries**\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "```\n",
    "- **`gensim`** provides the Word2Vec implementation.  \n",
    "- **`nltk`** helps tokenize (split) text into words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ujjwa\\miniconda3\\envs\\samarth\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\ujjwa\\miniconda3\\envs\\samarth\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\ujjwa\\miniconda3\\envs\\samarth\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ujjwa\\miniconda3\\envs\\samarth\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ujjwa\\miniconda3\\envs\\samarth\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ujjwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK tokenizer if not already installed\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Prepare the Corpus (Text Data)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"Dogs and cats are great pets\",\n",
    "    \"The pet store has dogs and cats\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is our **training data**. Word2Vec will learn from these sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'cat', 'sat', 'on', 'the', 'mat'], ['the', 'dog', 'barked', 'at', 'the', 'cat'], ['dogs', 'and', 'cats', 'are', 'great', 'pets'], ['the', 'pet', 'store', 'has', 'dogs', 'and', 'cats']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences into words\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Breaks each sentence into words** and converts them to **lowercase**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2Ô∏è‚É£ Training Word2Vec**\n",
    "Now we train **two models**:  \n",
    "- **CBOW (Continuous Bag of Words)**  \n",
    "- **Skip-gram**  \n",
    "\n",
    "#### **Step 4: Training CBOW Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CBOW model (sg=0 means CBOW)\n",
    "cbow_model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=3, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sentences=tokenized_corpus` ‚Üí Uses our tokenized data.  \n",
    "- `vector_size=50` ‚Üí Each word is represented by a **50-dimensional vector**.  \n",
    "- `window=2` ‚Üí Looks at **two words before and after** the target word for context.  \n",
    "- `min_count=1` ‚Üí Ignores words that appear less than once (here, all words are used).  \n",
    "- `sg=0` ‚Üí **Sets CBOW mode** (`sg=1` would be Skip-gram).  \n",
    "\n",
    "üìå **CBOW learns to predict the missing word based on surrounding words.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Training Skip-gram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=2, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Same parameters as CBOW** but with `sg=1`, which enables **Skip-gram**.  \n",
    "\n",
    "üìå **Skip-gram learns to predict surrounding words given a single word.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3Ô∏è‚É£ Using the Trained Word2Vec Model**\n",
    "Now that we have trained **CBOW and Skip-gram**, let's see how to use them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6: Get Word Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01723938  0.00733148  0.01037977  0.01148388  0.01493384 -0.01233535\n",
      "  0.00221123  0.01209456 -0.0056801  -0.01234705 -0.00082045 -0.0167379\n",
      " -0.01120002  0.01420908  0.00670508  0.01445134  0.01360049  0.01506148\n",
      " -0.00757831 -0.00112361  0.00469675 -0.00903806  0.01677746 -0.01971633\n",
      "  0.01352928  0.00582883 -0.00986566  0.00879638 -0.00347915  0.01342277\n",
      "  0.0199297  -0.00872489 -0.00119868 -0.01139127  0.00770164  0.00557325\n",
      "  0.01378215  0.01220219  0.01907699  0.01854683  0.01579614 -0.01397901\n",
      " -0.01831173 -0.00071151 -0.00619968  0.01578863  0.01187715 -0.00309133\n",
      "  0.00302193  0.00358008]\n"
     ]
    }
   ],
   "source": [
    "vector = cbow_model.wv['cat']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieves the **50-dimensional word vector** for `\"cat\"`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 7: Find Similar Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dogs', 0.16563552618026733), ('dog', 0.1551763415336609), ('pet', 0.14387421309947968), ('store', 0.1394207924604416), ('the', 0.12672513723373413), ('barked', 0.1211986094713211), ('has', 0.1051950454711914), ('great', 0.08872983604669571), ('sat', 0.032278481870889664), ('on', 0.02048538811504841)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = skipgram_model.wv.most_similar('cat')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finds words that are **semantically similar** to `\"cat\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "cbow_model.save(\"cbow.model\")\n",
    "skipgram_model.save(\"skipgram.model\")\n",
    "\n",
    "# Load models\n",
    "cbow_model = Word2Vec.load(\"cbow.model\")\n",
    "skipgram_model = Word2Vec.load(\"skipgram.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• **What Did We Do?**\n",
    "| Step | Action | Why? |\n",
    "|------|--------|------|\n",
    "| 1 | Tokenized text | So Word2Vec can process words |\n",
    "| 2 | Trained CBOW | Learns from surrounding words |\n",
    "| 3 | Trained Skip-gram | Predicts context words from a single word |\n",
    "| 4 | Extracted word vectors | To represent words numerically |\n",
    "| 5 | Found similar words | To check if embeddings make sense |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **CBOW vs. Skip-gram: Key Differences**\n",
    "| Feature | CBOW | Skip-gram |\n",
    "|---------|------|----------|\n",
    "| **Training Speed** | Faster | Slower |\n",
    "| **Works Well On** | Large datasets | Small datasets |\n",
    "| **Focus** | Predicts **target word** from context | Predicts **context words** from a target word |\n",
    "| **Performance on Rare Words** | Not good | Good |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samarth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
